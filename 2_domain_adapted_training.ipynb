{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjaK9bDdJyA4PM1GqSdJkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cw118/domain-adapted-nmt/blob/main/2_domain_adapted_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-adapted NMT\n",
        "\n",
        "## Training NMT models"
      ],
      "metadata": {
        "id": "K6TDCS_KZ3UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRyWkpHU3-w",
        "outputId": "6bf73a6c-9c49-4679-a467-eb117061ca4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.4.3-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.2,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.17 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2.1.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.10.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=2.0.1->OpenNMT-py) (2.1.3)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy->OpenNMT-py) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=2.0.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.4.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.24.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.6.1 sacrebleu-2.4.0 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into folder where prepared datasets were saved in the text processing step\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2ZuFaQZ2bc",
        "outputId": "e670d4e7-b4ca-46ff-a189-21cb682e21ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/domain-adapted-nmt/nmt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVFhh1Fp7mt",
        "outputId": "e5971b87-a491-47fd-d649-6f204b2f414c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/domain-adapted-nmt/nmt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the general/base model"
      ],
      "metadata": {
        "id": "Hql2_poQx3ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: corpora/en-de-general.en-filtered.en.subword.train\n",
        "        path_tgt: corpora/en-de-general.de-filtered.de.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: corpora/en-de-general.en-filtered.en.subword.dev\n",
        "        path_tgt: corpora/en-de-general.de-filtered.de.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.ende\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "lZ69S-sHaDfy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "testconf = '''# testconf.yaml\n",
        "\n",
        "\n",
        "## where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: corpora/en-de-general.en-filtered.en.subword.train\n",
        "        path_tgt: corpora/en-de-general.de-filtered.de.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: corpora/en-de-general.en-filtered.en.subword.dev\n",
        "        path_tgt: corpora/en-de-general.de-filtered.de.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.ende\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 200\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 1000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 300\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 500\n",
        "report_every: 50\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "# batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"testconf.yaml\", \"w+\") as testconf_yaml:\n",
        "  testconf_yaml.write(testconf)"
      ],
      "metadata": {
        "id": "1qWcD-JVnJH5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vDUluygfFYy",
        "outputId": "3d25746a-3fef-4378-b9fc-6afc44e579e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_-SZuTfI0Z",
        "outputId": "2a67a604-8b70-4e92-e37b-a724afea7a8f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 15:08:47.678508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 15:08:47.678572: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 15:08:47.679946: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 15:08:47.687513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 15:08:49.089854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-22 15:08:50.644749: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:08:50.645310: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:08:50.645520: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-22 15:08:51,690 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-22 15:08:51,690 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-22 15:09:06,479 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3964)\n",
            "\n",
            "[2024-01-22 15:09:06,741 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3959)\n",
            "\n",
            "[2024-01-22 15:09:06,789 INFO] Counters src: 5073\n",
            "[2024-01-22 15:09:06,789 INFO] Counters tgt: 6743\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 283, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 267, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 256, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config testconf.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "nKSqWZi1nkfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once runtime type is changed to GPU, check that the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do2zzPXFfdeZ",
        "outputId": "1039cfe0-2cec-4ee4-80fb-1bd7bce53cc3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-d5e9e4af-f8a2-85f3-9ecd-b101bbb4aed9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the GPU is visible to PyTorch\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0] / 1024**2, \"out of\", gpu_memory[1] / 1024**2)"
      ],
      "metadata": {
        "id": "8Tz1-U5zgENR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5eaa96d-d43b-441f-d226-6b2cf0f6bb8e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of 15102.0625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the models directory for a fresh start\n",
        "!rm -rf /content/drive/MyDrive/nmt/models"
      ],
      "metadata": {
        "id": "PO3lc2tdgMk4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8CZzr2XgS24",
        "outputId": "c381f50e-35e1-47a2-8b97-419d3b495050"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 15:10:10.964379: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 15:10:10.964442: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 15:10:10.966215: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 15:10:10.976501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 15:10:12.666750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-22 15:10:14.072785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:10:14.073213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:10:14.073372: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-22 15:10:14,791 INFO] Parsed 2 corpora from -data.\n",
            "[2024-01-22 15:10:15,164 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-22 15:10:16,179 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', 's', '▁', '▁of', ',', '▁and']\n",
            "[2024-01-22 15:10:16,180 INFO] The decoder start token is: <s>\n",
            "[2024-01-22 15:10:16,180 INFO] Building model...\n",
            "[2024-01-22 15:10:16,860 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-22 15:10:16,861 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-22 15:10:17,149 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5080, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6752, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=6752, bias=True)\n",
            ")\n",
            "[2024-01-22 15:10:17,153 INFO] encoder: 21488640\n",
            "[2024-01-22 15:10:17,153 INFO] decoder: 32106080\n",
            "[2024-01-22 15:10:17,153 INFO] * number of parameters: 53594720\n",
            "[2024-01-22 15:10:17,154 INFO] Trainable parameters = {'torch.float32': 53594720, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 15:10:17,154 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 15:10:17,155 INFO]  * src vocab size = 5080\n",
            "[2024-01-22 15:10:17,155 INFO]  * tgt vocab size = 6752\n",
            "[2024-01-22 15:10:17,760 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-01-22 15:10:17,760 INFO] Starting training on GPU: [0]\n",
            "[2024-01-22 15:10:17,760 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-01-22 15:10:17,761 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2024-01-22 15:10:31,972 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-01-22 15:10:47,960 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-01-22 15:12:05,608 INFO] Step 100/ 3000; acc: 5.1; ppl: 1388.2; xent: 7.2; lr: 0.00028; sents:   21825; bsz: 3346/3803/55; 12412/14106 tok/s;    108 sec;\n",
            "[2024-01-22 15:12:52,756 INFO] Step 200/ 3000; acc: 29.7; ppl: 127.7; xent: 4.8; lr: 0.00056; sents:   20801; bsz: 3383/3797/52; 28701/32214 tok/s;    155 sec;\n",
            "[2024-01-22 15:13:39,681 INFO] Step 300/ 3000; acc: 41.5; ppl:  51.1; xent: 3.9; lr: 0.00084; sents:   21319; bsz: 3410/3811/53; 29066/32484 tok/s;    202 sec;\n",
            "[2024-01-22 15:14:26,597 INFO] Step 400/ 3000; acc: 48.2; ppl:  34.8; xent: 3.5; lr: 0.00112; sents:   22262; bsz: 3359/3801/56; 28641/32409 tok/s;    249 sec;\n",
            "[2024-01-22 15:15:13,469 INFO] Step 500/ 3000; acc: 53.1; ppl:  26.8; xent: 3.3; lr: 0.00140; sents:   22661; bsz: 3351/3793/57; 28597/32368 tok/s;    296 sec;\n",
            "[2024-01-22 15:16:10,690 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18203)\n",
            "\n",
            "[2024-01-22 15:16:10,690 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-01-22 15:16:22,554 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-01-22 15:17:04,239 INFO] Step 600/ 3000; acc: 57.5; ppl:  21.6; xent: 3.1; lr: 0.00168; sents:   22539; bsz: 3376/3812/56; 12190/13766 tok/s;    406 sec;\n",
            "[2024-01-22 15:17:51,286 INFO] Step 700/ 3000; acc: 59.8; ppl:  19.3; xent: 3.0; lr: 0.00196; sents:   21834; bsz: 3340/3792/55; 28394/32241 tok/s;    454 sec;\n",
            "[2024-01-22 15:18:38,255 INFO] Step 800/ 3000; acc: 62.4; ppl:  17.0; xent: 2.8; lr: 0.00224; sents:   22232; bsz: 3362/3801/56; 28635/32375 tok/s;    500 sec;\n",
            "[2024-01-22 15:19:25,585 INFO] Step 900/ 3000; acc: 63.5; ppl:  16.1; xent: 2.8; lr: 0.00252; sents:   21426; bsz: 3391/3811/54; 28660/32209 tok/s;    548 sec;\n",
            "[2024-01-22 15:20:12,623 INFO] Step 1000/ 3000; acc: 64.3; ppl:  15.5; xent: 2.7; lr: 0.00279; sents:   21875; bsz: 3388/3809/55; 28809/32394 tok/s;    595 sec;\n",
            "[2024-01-22 15:20:18,743 INFO] valid stats calculation\n",
            "                           took: 6.1173412799835205 s.\n",
            "[2024-01-22 15:20:18,746 INFO] Train perplexity: 41.447\n",
            "[2024-01-22 15:20:18,748 INFO] Train accuracy: 48.5076\n",
            "[2024-01-22 15:20:18,748 INFO] Sentences processed: 218774\n",
            "[2024-01-22 15:20:18,748 INFO] Average bsz: 3371/3803/55\n",
            "[2024-01-22 15:20:18,748 INFO] Validation perplexity: 14.613\n",
            "[2024-01-22 15:20:18,748 INFO] Validation accuracy: 65.8345\n",
            "[2024-01-22 15:20:18,748 INFO] Model is improving ppl: inf --> 14.613.\n",
            "[2024-01-22 15:20:18,748 INFO] Model is improving acc: -inf --> 65.8345.\n",
            "[2024-01-22 15:20:18,753 INFO] Saving checkpoint models/model-base.ende_step_1000.pt\n",
            "[2024-01-22 15:22:32,786 INFO] Step 1100/ 3000; acc: 65.6; ppl:  14.5; xent: 2.7; lr: 0.00266; sents:   20852; bsz: 3389/3828/52; 9672/10924 tok/s;    735 sec;\n",
            "[2024-01-22 15:23:25,626 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18176)\n",
            "\n",
            "[2024-01-22 15:23:25,626 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-01-22 15:23:42,519 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-01-22 15:24:31,360 INFO] Step 1200/ 3000; acc: 66.6; ppl:  13.8; xent: 2.6; lr: 0.00255; sents:   23325; bsz: 3347/3781/58; 11291/12756 tok/s;    854 sec;\n",
            "[2024-01-22 15:25:18,529 INFO] Step 1300/ 3000; acc: 69.1; ppl:  12.1; xent: 2.5; lr: 0.00245; sents:   22260; bsz: 3379/3812/56; 28655/32329 tok/s;    901 sec;\n",
            "[2024-01-22 15:26:05,526 INFO] Step 1400/ 3000; acc: 70.1; ppl:  11.6; xent: 2.5; lr: 0.00236; sents:   21094; bsz: 3408/3822/53; 29006/32530 tok/s;    948 sec;\n",
            "[2024-01-22 15:26:52,845 INFO] Step 1500/ 3000; acc: 70.7; ppl:  11.3; xent: 2.4; lr: 0.00228; sents:   21746; bsz: 3363/3814/54; 28430/32238 tok/s;    995 sec;\n",
            "[2024-01-22 15:27:39,727 INFO] Step 1600/ 3000; acc: 72.3; ppl:  10.6; xent: 2.4; lr: 0.00221; sents:   21702; bsz: 3374/3813/54; 28785/32530 tok/s;   1042 sec;\n",
            "[2024-01-22 15:28:26,636 INFO] Step 1700/ 3000; acc: 73.3; ppl:  10.1; xent: 2.3; lr: 0.00214; sents:   22237; bsz: 3368/3801/56; 28723/32411 tok/s;   1089 sec;\n",
            "[2024-01-22 15:29:10,830 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18268)\n",
            "\n",
            "[2024-01-22 15:29:10,831 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-01-22 15:29:27,534 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-01-22 15:29:39,011 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-01-22 15:30:22,547 INFO] Step 1800/ 3000; acc: 74.2; ppl:   9.8; xent: 2.3; lr: 0.00208; sents:   22066; bsz: 3320/3753/55; 11456/12953 tok/s;   1205 sec;\n",
            "[2024-01-22 15:31:09,866 INFO] Step 1900/ 3000; acc: 75.7; ppl:   9.0; xent: 2.2; lr: 0.00203; sents:   22356; bsz: 3328/3778/56; 28134/31940 tok/s;   1252 sec;\n",
            "[2024-01-22 15:31:57,058 INFO] Step 2000/ 3000; acc: 76.2; ppl:   8.8; xent: 2.2; lr: 0.00198; sents:   22914; bsz: 3381/3806/57; 28658/32257 tok/s;   1299 sec;\n",
            "[2024-01-22 15:31:57,548 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=365)\n",
            "\n",
            "[2024-01-22 15:32:02,279 INFO] valid stats calculation\n",
            "                           took: 5.218698501586914 s.\n",
            "[2024-01-22 15:32:02,282 INFO] Train perplexity: 21.3912\n",
            "[2024-01-22 15:32:02,284 INFO] Train accuracy: 59.9363\n",
            "[2024-01-22 15:32:02,284 INFO] Sentences processed: 439326\n",
            "[2024-01-22 15:32:02,284 INFO] Average bsz: 3368/3802/55\n",
            "[2024-01-22 15:32:02,284 INFO] Validation perplexity: 8.80849\n",
            "[2024-01-22 15:32:02,284 INFO] Validation accuracy: 76.8938\n",
            "[2024-01-22 15:32:02,284 INFO] Model is improving ppl: 14.613 --> 8.80849.\n",
            "[2024-01-22 15:32:02,284 INFO] Model is improving acc: 65.8345 --> 76.8938.\n",
            "[2024-01-22 15:32:02,288 INFO] Saving checkpoint models/model-base.ende_step_2000.pt\n",
            "[2024-01-22 15:34:15,324 INFO] Step 2100/ 3000; acc: 77.1; ppl:   8.5; xent: 2.1; lr: 0.00193; sents:   22317; bsz: 3405/3801/56; 9849/10997 tok/s;   1438 sec;\n",
            "[2024-01-22 15:35:02,494 INFO] Step 2200/ 3000; acc: 77.2; ppl:   8.5; xent: 2.1; lr: 0.00188; sents:   22441; bsz: 3386/3814/56; 28717/32346 tok/s;   1485 sec;\n",
            "[2024-01-22 15:35:49,883 INFO] Step 2300/ 3000; acc: 77.9; ppl:   8.2; xent: 2.1; lr: 0.00184; sents:   20766; bsz: 3381/3818/52; 28540/32225 tok/s;   1532 sec;\n",
            "[2024-01-22 15:36:39,995 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18181)\n",
            "\n",
            "[2024-01-22 15:36:39,996 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2024-01-22 15:36:55,227 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2024-01-22 15:37:42,757 INFO] Step 2400/ 3000; acc: 78.4; ppl:   8.0; xent: 2.1; lr: 0.00180; sents:   20507; bsz: 3364/3815/51; 11920/13519 tok/s;   1645 sec;\n",
            "[2024-01-22 15:38:29,535 INFO] Step 2500/ 3000; acc: 79.7; ppl:   7.6; xent: 2.0; lr: 0.00177; sents:   22825; bsz: 3349/3767/57; 28636/32211 tok/s;   1692 sec;\n",
            "[2024-01-22 15:39:16,214 INFO] Step 2600/ 3000; acc: 80.1; ppl:   7.5; xent: 2.0; lr: 0.00173; sents:   21432; bsz: 3360/3798/54; 28793/32548 tok/s;   1738 sec;\n",
            "[2024-01-22 15:40:03,286 INFO] Step 2700/ 3000; acc: 80.2; ppl:   7.5; xent: 2.0; lr: 0.00170; sents:   22824; bsz: 3373/3806/57; 28659/32343 tok/s;   1786 sec;\n",
            "[2024-01-22 15:40:50,368 INFO] Step 2800/ 3000; acc: 80.3; ppl:   7.4; xent: 2.0; lr: 0.00167; sents:   20622; bsz: 3379/3833/52; 28712/32564 tok/s;   1833 sec;\n",
            "[2024-01-22 15:41:37,342 INFO] Step 2900/ 3000; acc: 80.9; ppl:   7.2; xent: 2.0; lr: 0.00164; sents:   21366; bsz: 3378/3812/53; 28766/32461 tok/s;   1880 sec;\n",
            "[2024-01-22 15:42:23,355 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=18230)\n",
            "\n",
            "[2024-01-22 15:42:23,355 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2024-01-22 15:42:38,916 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2024-01-22 15:43:31,437 INFO] Step 3000/ 3000; acc: 81.1; ppl:   7.2; xent: 2.0; lr: 0.00161; sents:   22502; bsz: 3345/3790/56; 11726/13289 tok/s;   1994 sec;\n",
            "[2024-01-22 15:43:31,933 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=365)\n",
            "\n",
            "[2024-01-22 15:43:43,217 INFO] valid stats calculation\n",
            "                           took: 11.778280019760132 s.\n",
            "[2024-01-22 15:43:43,222 INFO] Train perplexity: 15.2445\n",
            "[2024-01-22 15:43:43,224 INFO] Train accuracy: 66.3875\n",
            "[2024-01-22 15:43:43,224 INFO] Sentences processed: 656928\n",
            "[2024-01-22 15:43:43,224 INFO] Average bsz: 3369/3803/55\n",
            "[2024-01-22 15:43:43,224 INFO] Validation perplexity: 7.3532\n",
            "[2024-01-22 15:43:43,224 INFO] Validation accuracy: 81.3078\n",
            "[2024-01-22 15:43:43,224 INFO] Model is improving ppl: 8.80849 --> 7.3532.\n",
            "[2024-01-22 15:43:43,225 INFO] Model is improving acc: 76.8938 --> 81.3078.\n",
            "[2024-01-22 15:43:43,231 INFO] Saving checkpoint models/model-base.ende_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config testconf.yaml"
      ],
      "metadata": {
        "id": "4RRGnMRsnqyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in case Colab is suddenly unable to navigate through directories\n",
        "import os\n",
        "path = '/content/drive/MyDrive/domain-adapted-nmt/nmt'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "7HpP1TJI3QeM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -gpu 0 to use gpu\n",
        "!onmt_translate -model models/model-base.ende_step_3000.pt -src corpora/en-de-general.en-filtered.en.subword.test -output general-de.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdKHqwOJ1kKv",
        "outputId": "3d695385-3945-4bc7-cc2b-3549898d7f6c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 15:51:45.649895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 15:51:45.649962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 15:51:45.651747: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 15:51:45.662215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 15:51:46.905748: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-22 15:51:48.001538: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:51:48.002038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-22 15:51:48.002211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-22 15:51:48,424 INFO] Loading checkpoint from models/model-base.ende_step_3000.pt\n",
            "[2024-01-22 15:51:49,522 INFO] Loading data into the model\n",
            "[2024-01-22 15:56:36,484 INFO] PRED SCORE: -0.2865, PRED PPL: 1.33 NB SENTENCES: 5000\n",
            "Time w/o python interpreter load/terminate:  288.10639691352844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if on cpu\n",
        "!onmt_translate -model models/model-base.ende_step_1000.pt -src en-de-general.en-filtered.en.subword.test -output general-de.translated -min_length 1"
      ],
      "metadata": {
        "id": "zXWdvU_yuS1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-de.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVwRgj_O1nXJ",
        "outputId": "f6e6914c-430e-4965-dc9d-a6393a38e58b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁Fri ed en , ▁Sicherheit ▁und ▁Wieder ver ein igung ▁über ▁die ▁ko re anisch e ▁Hal bin sel\n",
            "▁Im ▁An schluss ▁an ▁mehrere ▁Miss ionen ▁in ▁der ▁Reg ion ▁be richtet e ▁ich ▁dem ▁Sicherheits rat ▁am ▁ 1 6 . ▁Ju n i , ▁dass ▁Is ra el ▁die ▁is ra el ischen ▁Tr uppe n ▁aus ▁Lib an on ▁zurück gezogen ▁hatte , ▁die ▁sich ▁aus ▁Lib an on ▁Rück zug s l ini e ▁im ▁Ein k lang ▁mit ▁der ▁Re s ol ut ion ▁ 4 2 5 ▁( 1 9 7 8 ) ▁des ▁Rat es ▁hervor ge ht .\n",
            "▁J ust i z ▁ist ▁ein ▁wesentlich er ▁Bestandteil ▁des ▁Recht s st a at s .\n",
            "▁Interna tionale r ▁Ge richt ▁für ▁Ru and a\n",
            "▁Eine ▁ erheblich e ▁Zu nahme ▁der ▁erforderlich en ▁Ressourcen ▁wird ▁erforderlich ▁sein , ▁um ▁die ▁k ünftige n ▁Her aus for der ungen ▁wirk sam ▁zu ▁bew ältig en .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q sentencepiece"
      ],
      "metadata": {
        "id": "f76FH1TE1quR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nmt\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xne3u-Ck5W-R",
        "outputId": "ac33a12d-c3bb-4862-ba1e-1fa1f905e6bc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/domain-adapted-nmt/nmt\n",
            "config.yaml\t       models\t       run\t     target.model   train.log\n",
            "corpora\t\t       MT-Preparation  source.model  target.vocab\n",
            "general-de.translated  nmt\t       source.vocab  testconf.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model general-de.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NugjpCmy5Hwo",
        "outputId": "92b36c88-c5da-4742-9481-d0fdc83f53c9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: general-de.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-de.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmqYi75g1sFX",
        "outputId": "0de21a66-cedf-4e56-b3ad-c322add6b408"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frieden, Sicherheit und Wiedervereinigung über die koreanische Halbinsel\n",
            "Im Anschluss an mehrere Missionen in der Region berichtete ich dem Sicherheitsrat am 16. Juni, dass Israel die israelischen Truppen aus Libanon zurückgezogen hatte, die sich aus Libanon Rückzugslinie im Einklang mit der Resolution 425 (1978) des Rates hervorgeht.\n",
            "Justiz ist ein wesentlicher Bestandteil des Rechtsstaats.\n",
            "Internationaler Gericht für Ruanda\n",
            "Eine erhebliche Zunahme der erforderlichen Ressourcen wird erforderlich sein, um die künftigen Herausforderungen wirksam zu bewältigen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target.model corpora/en-de-general.en-filtered.en.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_1RhBhL1tZF",
        "outputId": "50648763-8763-418d-d4fe-d2e266e95996"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: corpora/en-de-general.en-filtered.en.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove underscores: desubwording did not remove these successfully :/\n",
        "import re\n",
        "with open('corpora/en-de-general.en-filtered.en.subword.test.desubword', 'r') as ref, open('corpora/en-de-general-final.subword.test.desubword', 'w') as f:\n",
        "  for l in ref:\n",
        "    f.write(re.sub('▁', ' ', l).lstrip())"
      ],
      "metadata": {
        "id": "eeZ38yP2_Wus"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 corpora/en-de-general.en-filtered.en.subword.test.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG3qa1AX_o9H",
        "outputId": "d55ac916-94ad-443c-ea00-40bf27494235"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peace,▁security and reunification on the Korean peninsula\n",
            "▁Following▁several▁missions to the▁region by my Special Envoy, I▁reported to the▁Security▁Council on 16 June that Israeli▁forces had withdrawn from Lebanon in compliance with▁Council resolution 425 (1978).\n",
            "Justice is a vital▁component of the▁rule of▁law.\n",
            "International Tribunal for Rwanda\n",
            "A▁significant▁increase in▁resources will be▁required to address▁future challenges▁effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 corpora/en-de-general-final.subword.test.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8xGflVvDoBN",
        "outputId": "77d72cf6-ba7f-4e67-ff5a-85999e57dbc4"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peace, security and reunification on the Korean peninsula\n",
            "Following several missions to the region by my Special Envoy, I reported to the Security Council on 16 June that Israeli forces had withdrawn from Lebanon in compliance with Council resolution 425 (1978).\n",
            "Justice is a vital component of the rule of law.\n",
            "International Tribunal for Rwanda\n",
            "A significant increase in resources will be required to address future challenges effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test bleu for baseline score\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ],
      "metadata": {
        "id": "T5bw8Qqp1uzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 compute-bleu.py corpora/en-de-general-final.subword.test.desubword general-de.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZidBkQGF5xAY",
        "outputId": "226e840b-ad47-4309-c514-bd3f9f6558ec"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Peace, security and reunification on the Korean peninsula\n",
            "MTed 1st sentence: Frieden, Sicherheit und Wiedervereinigung über die koreanische Halbinsel\n",
            "BLEU:  4.342904968955745\n"
          ]
        }
      ]
    }
  ]
}