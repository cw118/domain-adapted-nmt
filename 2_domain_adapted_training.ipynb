{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNU3fEs0ZmJQS0945x52tD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cw118/domain-adapted-nmt/blob/main/2_domain_adapted_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-adapted NMT\n",
        "\n",
        "## Training NMT models"
      ],
      "metadata": {
        "id": "K6TDCS_KZ3UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRyWkpHU3-w",
        "outputId": "05d4cb25-7a71-4f04-d385-1caad05a798e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.4.3-py3-none-any.whl (257 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/257.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.2,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.17 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2.1.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.10.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=2.0.1->OpenNMT-py) (2.1.3)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy->OpenNMT-py) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=2.0.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.4.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.24.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.6.1 sacrebleu-2.4.0 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into folder where prepared datasets were saved in the text processing step\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/nmt/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2ZuFaQZ2bc",
        "outputId": "9961cc04-73c9-4ad7-d003-6f44a44aedd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/nmt/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "config = '''# config.yaml\n",
        "\n",
        "\n",
        "## where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: en-de-general.en-filtered.en.subword.train\n",
        "        path_tgt: en-de-general.de-filtered.de.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: en-de-general.en-filtered.en.subword.dev\n",
        "        path_tgt: en-de-general.de-filtered.de.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.ende\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 1000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 3000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 1000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "lZ69S-sHaDfy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "testconf = '''# testconf.yaml\n",
        "\n",
        "\n",
        "## where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: en-de-general.en-filtered.en.subword.train\n",
        "        path_tgt: en-de-general.de-filtered.de.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: en-de-general.en-filtered.en.subword.dev\n",
        "        path_tgt: en-de-general.de-filtered.de.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "src_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source.model\n",
        "tgt_subword_model: target.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.ende\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 200\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 3\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 1000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 300\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 500\n",
        "report_every: 50\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "# batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"testconf.yaml\", \"w+\") as testconf_yaml:\n",
        "  testconf_yaml.write(testconf)"
      ],
      "metadata": {
        "id": "1qWcD-JVnJH5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vDUluygfFYy",
        "outputId": "e72695ce-81cb-4144-9682-fbe1d25d7177"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH_-SZuTfI0Z",
        "outputId": "165060bf-041a-4b7d-df91-d1329cd322aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 01:18:04.817427: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 01:18:04.817487: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 01:18:04.819580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 01:18:04.831993: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 01:18:06.298461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-22 01:18:08,627 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-22 01:18:08,627 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-22 01:18:24,444 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3959)\n",
            "\n",
            "[2024-01-22 01:18:24,601 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3964)\n",
            "\n",
            "[2024-01-22 01:18:24,638 INFO] Counters src: 5073\n",
            "[2024-01-22 01:18:24,639 INFO] Counters tgt: 6743\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 283, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 267, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 256, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config testconf.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa4acfa-7935-4001-802a-a01c3d7eaacf",
        "id": "nKSqWZi1nkfy"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 00:44:30.614755: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 00:44:30.614819: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 00:44:30.617942: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 00:44:30.627397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 00:44:31.941757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-22 00:44:34,071 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-22 00:44:34,071 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-22 00:44:49,761 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3964)\n",
            "\n",
            "[2024-01-22 00:44:49,834 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3959)\n",
            "\n",
            "[2024-01-22 00:44:49,867 INFO] Counters src: 5073\n",
            "[2024-01-22 00:44:49,868 INFO] Counters tgt: 6743\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 283, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 267, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 256, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once runtime type is changed to GPU, check that the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do2zzPXFfdeZ",
        "outputId": "975dbc50-1184-4849-b499-16e8df6f870a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the GPU is visible to PyTorch\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0] / 1024**2, \"out of\", gpu_memory[1] / 1024**2)"
      ],
      "metadata": {
        "id": "8Tz1-U5zgENR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the models directory for a fresh start\n",
        "!rm -rf /content/drive/MyDrive/nmt/models"
      ],
      "metadata": {
        "id": "PO3lc2tdgMk4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8CZzr2XgS24",
        "outputId": "a607f636-bb5d-4a8e-9de1-f16bc7722fe0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 00:24:09.214569: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 00:24:09.214624: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 00:24:09.216079: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 00:24:09.224242: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 00:24:10.526090: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-22 00:24:13,513 INFO] Parsed 2 corpora from -data.\n",
            "[2024-01-22 00:24:13,514 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-22 00:24:13,566 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', 's', '▁', '▁of', ',', '▁and']\n",
            "[2024-01-22 00:24:13,567 INFO] The decoder start token is: <s>\n",
            "[2024-01-22 00:24:13,567 INFO] Building model...\n",
            "[2024-01-22 00:24:15,647 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-22 00:24:15,648 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-22 00:24:15,655 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5080, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6752, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=6752, bias=True)\n",
            ")\n",
            "[2024-01-22 00:24:15,660 INFO] encoder: 21488640\n",
            "[2024-01-22 00:24:15,661 INFO] decoder: 32106080\n",
            "[2024-01-22 00:24:15,661 INFO] * number of parameters: 53594720\n",
            "[2024-01-22 00:24:15,662 INFO] Trainable parameters = {'torch.float32': 53594720, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 00:24:15,662 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 00:24:15,662 INFO]  * src vocab size = 5080\n",
            "[2024-01-22 00:24:15,662 INFO]  * tgt vocab size = 6752\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "[2024-01-22 00:24:16,348 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-01-22 00:24:16,349 INFO] Starting training on CPU, could be very slow\n",
            "[2024-01-22 00:24:16,349 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-01-22 00:24:16,349 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2024-01-22 00:24:36,227 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-01-22 00:24:53,224 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config testconf.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30468763-36be-4338-caf8-85119baa7155",
        "id": "4RRGnMRsnqyZ"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-22 01:18:30.355143: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-22 01:18:30.355202: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-22 01:18:30.356602: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-22 01:18:30.364653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-22 01:18:31.680167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-22 01:18:34,228 INFO] Parsed 2 corpora from -data.\n",
            "[2024-01-22 01:18:34,228 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-22 01:18:34,257 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', 's', '▁', '▁of', ',', '▁and']\n",
            "[2024-01-22 01:18:34,258 INFO] The decoder start token is: <s>\n",
            "[2024-01-22 01:18:34,258 INFO] Building model...\n",
            "[2024-01-22 01:18:35,260 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-22 01:18:35,260 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-22 01:18:35,264 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(5080, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(6752, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=6752, bias=True)\n",
            ")\n",
            "[2024-01-22 01:18:35,267 INFO] encoder: 21488640\n",
            "[2024-01-22 01:18:35,267 INFO] decoder: 32106080\n",
            "[2024-01-22 01:18:35,267 INFO] * number of parameters: 53594720\n",
            "[2024-01-22 01:18:35,268 INFO] Trainable parameters = {'torch.float32': 53594720, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 01:18:35,268 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-22 01:18:35,268 INFO]  * src vocab size = 5080\n",
            "[2024-01-22 01:18:35,269 INFO]  * tgt vocab size = 6752\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "[2024-01-22 01:18:35,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-01-22 01:18:35,767 INFO] Starting training on CPU, could be very slow\n",
            "[2024-01-22 01:18:35,767 INFO] Start training loop and validate every 300 steps...\n",
            "[2024-01-22 01:18:35,767 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2024-01-22 01:18:56,124 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-01-22 01:19:20,114 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "[2024-01-22 01:22:06,497 INFO] Step 50/ 1000; acc: 2.6; ppl: 2058.0; xent: 7.6; lr: 0.00040; sents:     222; bsz:   69/  78/ 1;  65/ 74 tok/s;    211 sec;\n",
            "[2024-01-22 01:24:11,967 INFO] Step 100/ 1000; acc: 3.7; ppl: 863.5; xent: 6.8; lr: 0.00080; sents:     228; bsz:   71/  81/ 1; 114/129 tok/s;    336 sec;\n",
            "[2024-01-22 01:26:13,604 INFO] Step 150/ 1000; acc: 5.1; ppl: 758.5; xent: 6.6; lr: 0.00119; sents:     215; bsz:   69/  78/ 1; 113/128 tok/s;    458 sec;\n",
            "[2024-01-22 01:28:29,424 INFO] Step 200/ 1000; acc: 8.7; ppl: 583.1; xent: 6.4; lr: 0.00159; sents:     210; bsz:   73/  84/ 1; 107/123 tok/s;    594 sec;\n",
            "[2024-01-22 01:28:29,429 INFO] Saving checkpoint models/model-base.ende_step_200.pt\n",
            "[2024-01-22 01:31:02,819 INFO] Step 250/ 1000; acc: 12.3; ppl: 441.7; xent: 6.1; lr: 0.00198; sents:     225; bsz:   71/  80/ 1;  92/104 tok/s;    747 sec;\n",
            "[2024-01-22 01:33:46,973 INFO] Step 300/ 1000; acc: 12.6; ppl: 402.1; xent: 6.0; lr: 0.00238; sents:     217; bsz:   71/  80/ 1;  86/ 98 tok/s;    911 sec;\n",
            "[2024-01-22 01:45:18,222 INFO] valid stats calculation\n",
            "                           took: 691.2467358112335 s.\n",
            "[2024-01-22 01:45:18,224 INFO] Train perplexity: 715.082\n",
            "[2024-01-22 01:45:18,224 INFO] Train accuracy: 7.55902\n",
            "[2024-01-22 01:45:18,224 INFO] Sentences processed: 1317\n",
            "[2024-01-22 01:45:18,224 INFO] Average bsz:   71/  80/ 1\n",
            "[2024-01-22 01:45:18,224 INFO] Validation perplexity: 374.624\n",
            "[2024-01-22 01:45:18,225 INFO] Validation accuracy: 13.7785\n",
            "[2024-01-22 01:45:18,225 INFO] Model is improving ppl: inf --> 374.624.\n",
            "[2024-01-22 01:45:18,225 INFO] Model is improving acc: -inf --> 13.7785.\n",
            "[2024-01-22 01:48:18,960 INFO] Step 350/ 1000; acc: 12.8; ppl: 378.6; xent: 5.9; lr: 0.00277; sents:     227; bsz:   71/  80/ 1;  16/ 18 tok/s;   1783 sec;\n",
            "[2024-01-22 01:51:38,118 INFO] Step 400/ 1000; acc: 12.4; ppl: 377.7; xent: 5.9; lr: 0.00317; sents:     214; bsz:   71/  81/ 1;  72/ 81 tok/s;   1982 sec;\n",
            "[2024-01-22 01:51:38,122 INFO] Saving checkpoint models/model-base.ende_step_400.pt\n",
            "[2024-01-22 01:55:31,924 INFO] Step 450/ 1000; acc: 13.5; ppl: 364.9; xent: 5.9; lr: 0.00357; sents:     218; bsz:   70/  80/ 1;  60/ 68 tok/s;   2216 sec;\n",
            "[2024-01-22 01:59:40,157 INFO] Step 500/ 1000; acc: 13.1; ppl: 355.5; xent: 5.9; lr: 0.00395; sents:     211; bsz:   70/  80/ 1;  56/ 64 tok/s;   2464 sec;\n",
            "[2024-01-22 02:04:02,646 INFO] Step 550/ 1000; acc: 12.2; ppl: 385.8; xent: 6.0; lr: 0.00377; sents:     216; bsz:   68/  76/ 1;  52/ 58 tok/s;   2727 sec;\n",
            "[2024-01-22 02:08:54,819 INFO] Step 600/ 1000; acc: 12.7; ppl: 368.3; xent: 5.9; lr: 0.00361; sents:     225; bsz:   74/  83/ 1;  50/ 57 tok/s;   3019 sec;\n",
            "[2024-01-22 02:08:55,344 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=365)\n",
            "\n",
            "[2024-01-22 02:20:56,195 INFO] valid stats calculation\n",
            "                           took: 721.3743891716003 s.\n",
            "[2024-01-22 02:20:56,198 INFO] Train perplexity: 515.523\n",
            "[2024-01-22 02:20:56,198 INFO] Train accuracy: 10.1761\n",
            "[2024-01-22 02:20:56,198 INFO] Sentences processed: 2628\n",
            "[2024-01-22 02:20:56,198 INFO] Average bsz:   71/  80/ 1\n",
            "[2024-01-22 02:20:56,198 INFO] Validation perplexity: 342.147\n",
            "[2024-01-22 02:20:56,198 INFO] Validation accuracy: 13.2406\n",
            "[2024-01-22 02:20:56,198 INFO] Stalled patience: 3/4\n",
            "[2024-01-22 02:20:56,204 INFO] Saving checkpoint models/model-base.ende_step_600.pt\n",
            "[2024-01-22 02:25:41,921 INFO] Step 650/ 1000; acc: 13.0; ppl: 347.5; xent: 5.9; lr: 0.00346; sents:     235; bsz:   68/  78/ 1;  14/ 15 tok/s;   4026 sec;\n",
            "[2024-01-22 02:30:23,392 INFO] Step 700/ 1000; acc: 13.1; ppl: 345.3; xent: 5.8; lr: 0.00334; sents:     219; bsz:   71/  79/ 1;  51/ 56 tok/s;   4308 sec;\n",
            "[2024-01-22 02:34:42,061 INFO] Step 750/ 1000; acc: 11.9; ppl: 395.7; xent: 6.0; lr: 0.00323; sents:     222; bsz:   67/  74/ 1;  52/ 57 tok/s;   4566 sec;\n",
            "[2024-01-22 02:39:02,899 INFO] Step 800/ 1000; acc: 13.2; ppl: 332.9; xent: 5.8; lr: 0.00312; sents:     232; bsz:   69/  76/ 1;  53/ 59 tok/s;   4827 sec;\n",
            "[2024-01-22 02:39:02,906 INFO] Saving checkpoint models/model-base.ende_step_800.pt\n",
            "[2024-01-22 02:43:35,716 INFO] Step 850/ 1000; acc: 12.6; ppl: 356.7; xent: 5.9; lr: 0.00303; sents:     221; bsz:   68/  76/ 1;  50/ 55 tok/s;   5100 sec;\n",
            "[2024-01-22 02:47:44,264 INFO] Step 900/ 1000; acc: 11.8; ppl: 400.2; xent: 6.0; lr: 0.00294; sents:     215; bsz:   66/  74/ 1;  53/ 60 tok/s;   5348 sec;\n",
            "[2024-01-22 02:47:44,798 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=365)\n",
            "\n",
            "[2024-01-22 02:59:03,977 INFO] valid stats calculation\n",
            "                           took: 679.7110002040863 s.\n",
            "[2024-01-22 02:59:03,978 INFO] Train perplexity: 459.775\n",
            "[2024-01-22 02:59:03,979 INFO] Train accuracy: 10.9627\n",
            "[2024-01-22 02:59:03,979 INFO] Sentences processed: 3972\n",
            "[2024-01-22 02:59:03,979 INFO] Average bsz:   70/  79/ 1\n",
            "[2024-01-22 02:59:03,979 INFO] Validation perplexity: 359.399\n",
            "[2024-01-22 02:59:03,979 INFO] Validation accuracy: 12.3599\n",
            "[2024-01-22 02:59:03,979 INFO] Stalled patience: 2/4\n",
            "[2024-01-22 03:03:17,698 INFO] Step 950/ 1000; acc: 11.8; ppl: 378.8; xent: 5.9; lr: 0.00287; sents:     227; bsz:   68/  76/ 1;  14/ 16 tok/s;   6282 sec;\n",
            "[2024-01-22 03:07:16,773 INFO] Step 1000/ 1000; acc: 13.1; ppl: 363.1; xent: 5.9; lr: 0.00279; sents:     221; bsz:   65/  73/ 1;  54/ 61 tok/s;   6521 sec;\n",
            "[2024-01-22 03:07:16,778 INFO] Saving checkpoint models/model-base.ende_step_1000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -gpu 0 (to use gpu)\n",
        "!onmt_translate -model models/model-base.ende_step_1000.pt -src en-de-general.en-filtered.en.subword.test -output general-de.translated -min_length 1"
      ],
      "metadata": {
        "id": "DGdb43HygU_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-de.translated"
      ],
      "metadata": {
        "id": "m_VYgYVrgzYI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir nmt\n",
        "%cd nmt\n",
        "!git clone https://github.com/ymoslem/MT-Preparation.git\n",
        "!pip3 install -r MT-Preparation/requirements.txt"
      ],
      "metadata": {
        "id": "Fh-ZJhrpJUpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q sentencepiece\n",
        "\n",
        "# desubword translated file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target.model general-de.translated"
      ],
      "metadata": {
        "id": "yyqEvzwSg2LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-de.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdRfkQCBhEPB",
        "outputId": "ea36d9a1-5144-401f-9546-4f81bda08aa7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'general-de.translated.desubword' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target.model en-de-general.en-filtered.en.subword.test"
      ],
      "metadata": {
        "id": "QekTcsxyhGpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test bleu for fun\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu\n",
        "!python3 compute-bleu.py en-de-general.en-filtered.en.subword.test.desubword general-de.translated.desubword"
      ],
      "metadata": {
        "id": "9GUiWbwGjn4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}