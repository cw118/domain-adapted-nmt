{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxuFEj8K0ZHLXD1Bt2yhhH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cw118/domain-adapted-nmt/blob/main/2_fr_domain_adapted_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Domain-adapted NMT\n",
        "\n",
        "## Training NMT models"
      ],
      "metadata": {
        "id": "K6TDCS_KZ3UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRyWkpHU3-w",
        "outputId": "4991dd4b-a0e3-4904-f151-639e76ac919a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.4.3-py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.2,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.17 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2.1.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.10.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=2.0.1->OpenNMT-py) (2.1.3)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy->OpenNMT-py) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=2.0.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.4.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.24.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.6.1 sacrebleu-2.4.0 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into folder where prepared datasets were saved in the text processing step\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2ZuFaQZ2bc",
        "outputId": "249bf8c9-e379-4d0d-9862-f7c415bdffdb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/domain-adapted-nmt/nmt-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVFhh1Fp7mt",
        "outputId": "576ba0f9-19be-473e-ad04-3ae79a5f4e63"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/domain-adapted-nmt/nmt-tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the general/base model"
      ],
      "metadata": {
        "id": "Hql2_poQx3ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "config = '''# config.yaml\n",
        "\n",
        "# where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "        path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.dev\n",
        "        path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source-general.model\n",
        "tgt_subword_model: target-general.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.enfr\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 2000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 4\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 10000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 2000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 2000\n",
        "report_every: 200\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "lZ69S-sHaDfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vDUluygfFYy",
        "outputId": "6d486213-1fa9-476a-a2ea-d4c827b54c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "SH_-SZuTfI0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a7a7c2-c3d2-4ed2-e798-f527901bea41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 05:58:34.320451: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:58:34.320533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:58:34.322444: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:58:34.333672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 05:58:36.159706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 05:58:37.553520: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:58:37.554007: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:58:37.554221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 05:58:37,993 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-24 05:58:37,993 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-24 05:59:10,219 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=839)\n",
            "\n",
            "[2024-01-24 05:59:10,277 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=785)\n",
            "\n",
            "[2024-01-24 05:59:10,463 INFO] Counters src: 49571\n",
            "[2024-01-24 05:59:10,463 INFO] Counters tgt: 49999\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_build_vocab\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 283, in main\n",
            "    build_vocab_main(opts)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 267, in build_vocab_main\n",
            "    save_counter(src_counter, opts.src_vocab)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/bin/build_vocab.py\", line 256, in save_counter\n",
            "    check_path(save_path, exist_ok=opts.overwrite, log=logger.warning)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onmt/utils/misc.py\", line 47, in check_path\n",
            "    raise IOError(f\"path {path} exists, stop.\")\n",
            "OSError: path run/source.vocab exists, stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once runtime type is changed to GPU, check that the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do2zzPXFfdeZ",
        "outputId": "a2049528-4269-4410-dc8a-720ae603db8d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-698f9a39-0e14-e3b2-f11f-cb9474926d82)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the GPU is visible to PyTorch\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0] / 1024**2, \"out of\", gpu_memory[1] / 1024**2)"
      ],
      "metadata": {
        "id": "8Tz1-U5zgENR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f522d3-9b4a-46d7-e2c8-09e1c09055eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of 15102.0625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the models directory for a fresh start\n",
        "!rm -rf /content/drive/MyDrive/nmt-tools/models"
      ],
      "metadata": {
        "id": "PO3lc2tdgMk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "id": "t8CZzr2XgS24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588c6048-6e75-4e29-a4fc-7a0a6fd50ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 00:20:52.369413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 00:20:52.369470: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 00:20:52.371983: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 00:20:52.382420: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 00:20:53.947009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 00:20:54.978334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 00:20:54.978762: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 00:20:54.978927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 00:20:55,670 INFO] Parsed 2 corpora from -data.\n",
            "[2024-01-24 00:20:55,672 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-24 00:20:55,881 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', '▁of', ',', '.', '▁and', '▁to']\n",
            "[2024-01-24 00:20:55,881 INFO] The decoder start token is: <s>\n",
            "[2024-01-24 00:20:55,882 INFO] Building model...\n",
            "[2024-01-24 00:20:57,433 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-24 00:20:57,434 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-24 00:20:57,798 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(49576, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=50000, bias=True)\n",
            ")\n",
            "[2024-01-24 00:20:57,801 INFO] encoder: 44270592\n",
            "[2024-01-24 00:20:57,801 INFO] decoder: 76435280\n",
            "[2024-01-24 00:20:57,801 INFO] * number of parameters: 120705872\n",
            "[2024-01-24 00:20:57,802 INFO] Trainable parameters = {'torch.float32': 120705872, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-24 00:20:57,802 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-24 00:20:57,802 INFO]  * src vocab size = 49576\n",
            "[2024-01-24 00:20:57,802 INFO]  * tgt vocab size = 50000\n",
            "[2024-01-24 00:20:58,386 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-01-24 00:20:58,387 INFO] Starting training on GPU: [0]\n",
            "[2024-01-24 00:20:58,387 INFO] Start training loop and validate every 2000 steps...\n",
            "[2024-01-24 00:20:58,387 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=192))\n",
            "[2024-01-24 00:23:59,844 INFO] Step 200/10000; acc: 7.1; ppl: 4575.9; xent: 8.4; lr: 0.00020; sents:   70622; bsz: 3110/3814/88; 13712/16814 tok/s;    181 sec;\n",
            "[2024-01-24 00:26:34,490 INFO] Step 400/10000; acc: 21.2; ppl: 316.7; xent: 5.8; lr: 0.00040; sents:   75784; bsz: 3132/3824/95; 16203/19781 tok/s;    336 sec;\n",
            "[2024-01-24 00:29:09,414 INFO] Step 600/10000; acc: 29.1; ppl: 150.9; xent: 5.0; lr: 0.00059; sents:   72777; bsz: 3076/3811/91; 15885/19681 tok/s;    491 sec;\n",
            "[2024-01-24 00:30:46,026 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=995)\n",
            "\n",
            "[2024-01-24 00:30:46,027 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-01-24 00:32:29,641 INFO] Step 800/10000; acc: 38.6; ppl:  83.5; xent: 4.4; lr: 0.00079; sents:   75764; bsz: 3093/3804/95; 12358/15197 tok/s;    691 sec;\n",
            "[2024-01-24 00:35:05,871 INFO] Step 1000/10000; acc: 46.6; ppl:  53.3; xent: 4.0; lr: 0.00099; sents:   71723; bsz: 3132/3828/90; 16038/19603 tok/s;    847 sec;\n",
            "[2024-01-24 00:37:41,708 INFO] Step 1200/10000; acc: 53.1; ppl:  38.0; xent: 3.6; lr: 0.00119; sents:   75651; bsz: 3115/3798/95; 15990/19498 tok/s;   1003 sec;\n",
            "[2024-01-24 00:40:17,654 INFO] Step 1400/10000; acc: 57.2; ppl:  30.5; xent: 3.4; lr: 0.00138; sents:   73073; bsz: 3092/3840/91; 15860/19700 tok/s;   1159 sec;\n",
            "[2024-01-24 00:43:37,622 INFO] Step 1600/10000; acc: 60.2; ppl:  25.8; xent: 3.2; lr: 0.00158; sents:   73824; bsz: 3105/3822/92; 12423/15290 tok/s;   1359 sec;\n",
            "[2024-01-24 00:46:13,266 INFO] Step 1800/10000; acc: 61.5; ppl:  24.0; xent: 3.2; lr: 0.00178; sents:   72547; bsz: 3100/3806/91; 15935/19564 tok/s;   1515 sec;\n",
            "[2024-01-24 00:48:49,109 INFO] Step 2000/10000; acc: 63.0; ppl:  22.2; xent: 3.1; lr: 0.00198; sents:   72874; bsz: 3110/3828/91; 15964/19653 tok/s;   1671 sec;\n",
            "[2024-01-24 00:48:53,489 INFO] valid stats calculation\n",
            "                           took: 4.37697434425354 s.\n",
            "[2024-01-24 00:48:53,490 INFO] Train perplexity: 82.9181\n",
            "[2024-01-24 00:48:53,490 INFO] Train accuracy: 43.7635\n",
            "[2024-01-24 00:48:53,490 INFO] Sentences processed: 734639\n",
            "[2024-01-24 00:48:53,490 INFO] Average bsz: 3107/3818/92\n",
            "[2024-01-24 00:48:53,490 INFO] Validation perplexity: 20.3221\n",
            "[2024-01-24 00:48:53,490 INFO] Validation accuracy: 64.7767\n",
            "[2024-01-24 00:48:53,491 INFO] Model is improving ppl: inf --> 20.3221.\n",
            "[2024-01-24 00:48:53,491 INFO] Model is improving acc: -inf --> 64.7767.\n",
            "[2024-01-24 00:48:53,508 INFO] Saving checkpoint models/model-base.enfr_step_2000.pt\n",
            "[2024-01-24 00:50:43,533 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2033)\n",
            "\n",
            "[2024-01-24 00:50:43,533 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-01-24 00:52:21,145 INFO] Step 2200/10000; acc: 64.1; ppl:  20.7; xent: 3.0; lr: 0.00188; sents:   73551; bsz: 3113/3794/92; 11745/14313 tok/s;   1883 sec;\n",
            "[2024-01-24 00:54:57,185 INFO] Step 2400/10000; acc: 65.9; ppl:  18.7; xent: 2.9; lr: 0.00180; sents:   74037; bsz: 3107/3798/93; 15929/19470 tok/s;   2039 sec;\n",
            "[2024-01-24 00:57:33,781 INFO] Step 2600/10000; acc: 67.0; ppl:  17.6; xent: 2.9; lr: 0.00173; sents:   75129; bsz: 3117/3828/94; 15926/19556 tok/s;   2195 sec;\n",
            "[2024-01-24 01:00:10,238 INFO] Step 2800/10000; acc: 67.6; ppl:  16.9; xent: 2.8; lr: 0.00167; sents:   73292; bsz: 3106/3830/92; 15884/19582 tok/s;   2352 sec;\n",
            "[2024-01-24 01:00:53,309 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=990)\n",
            "\n",
            "[2024-01-24 01:00:53,309 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-01-24 01:03:28,945 INFO] Step 3000/10000; acc: 69.0; ppl:  15.5; xent: 2.7; lr: 0.00161; sents:   71334; bsz: 3109/3823/89; 12517/15390 tok/s;   2551 sec;\n",
            "[2024-01-24 01:06:04,806 INFO] Step 3200/10000; acc: 69.6; ppl:  15.0; xent: 2.7; lr: 0.00156; sents:   73478; bsz: 3106/3824/92; 15943/19628 tok/s;   2706 sec;\n",
            "[2024-01-24 01:08:40,662 INFO] Step 3400/10000; acc: 69.8; ppl:  14.9; xent: 2.7; lr: 0.00152; sents:   75563; bsz: 3097/3805/94; 15898/19530 tok/s;   2862 sec;\n",
            "[2024-01-24 01:12:02,548 INFO] Step 3600/10000; acc: 70.7; ppl:  14.1; xent: 2.6; lr: 0.00147; sents:   71043; bsz: 3118/3831/89; 12356/15182 tok/s;   3064 sec;\n",
            "[2024-01-24 01:14:38,421 INFO] Step 3800/10000; acc: 71.6; ppl:  13.3; xent: 2.6; lr: 0.00143; sents:   75985; bsz: 3108/3841/95; 15953/19715 tok/s;   3220 sec;\n",
            "[2024-01-24 01:17:13,920 INFO] Step 4000/10000; acc: 71.5; ppl:  13.4; xent: 2.6; lr: 0.00140; sents:   70686; bsz: 3098/3810/88; 15938/19601 tok/s;   3376 sec;\n",
            "[2024-01-24 01:17:14,086 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11)\n",
            "\n",
            "[2024-01-24 01:17:17,354 INFO] valid stats calculation\n",
            "                           took: 3.432727575302124 s.\n",
            "[2024-01-24 01:17:17,356 INFO] Train perplexity: 36.2422\n",
            "[2024-01-24 01:17:17,356 INFO] Train accuracy: 56.2234\n",
            "[2024-01-24 01:17:17,356 INFO] Sentences processed: 1.46874e+06\n",
            "[2024-01-24 01:17:17,356 INFO] Average bsz: 3107/3818/92\n",
            "[2024-01-24 01:17:17,356 INFO] Validation perplexity: 13.7608\n",
            "[2024-01-24 01:17:17,356 INFO] Validation accuracy: 71.7919\n",
            "[2024-01-24 01:17:17,356 INFO] Model is improving ppl: 20.3221 --> 13.7608.\n",
            "[2024-01-24 01:17:17,356 INFO] Model is improving acc: 64.7767 --> 71.7919.\n",
            "[2024-01-24 01:17:17,372 INFO] Saving checkpoint models/model-base.enfr_step_4000.pt\n",
            "[2024-01-24 01:20:12,043 INFO] Step 4200/10000; acc: 71.7; ppl:  13.3; xent: 2.6; lr: 0.00136; sents:   73979; bsz: 3126/3815/92; 14040/17134 tok/s;   3554 sec;\n",
            "[2024-01-24 01:21:04,200 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2022)\n",
            "\n",
            "[2024-01-24 01:21:04,200 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-01-24 01:23:38,306 INFO] Step 4400/10000; acc: 72.6; ppl:  12.6; xent: 2.5; lr: 0.00133; sents:   76180; bsz: 3083/3801/95; 11958/14742 tok/s;   3760 sec;\n",
            "[2024-01-24 01:26:14,385 INFO] Step 4600/10000; acc: 72.9; ppl:  12.4; xent: 2.5; lr: 0.00130; sents:   73748; bsz: 3094/3800/92; 15857/19476 tok/s;   3916 sec;\n",
            "[2024-01-24 01:28:50,253 INFO] Step 4800/10000; acc: 73.0; ppl:  12.3; xent: 2.5; lr: 0.00128; sents:   73055; bsz: 3104/3823/91; 15932/19624 tok/s;   4072 sec;\n",
            "[2024-01-24 01:32:11,202 INFO] Step 5000/10000; acc: 73.5; ppl:  12.0; xent: 2.5; lr: 0.00125; sents:   73854; bsz: 3150/3834/92; 12540/15264 tok/s;   4273 sec;\n",
            "[2024-01-24 01:34:47,237 INFO] Step 5200/10000; acc: 74.2; ppl:  11.5; xent: 2.4; lr: 0.00123; sents:   73108; bsz: 3111/3813/91; 15949/19552 tok/s;   4429 sec;\n",
            "[2024-01-24 01:37:23,180 INFO] Step 5400/10000; acc: 74.1; ppl:  11.6; xent: 2.4; lr: 0.00120; sents:   73962; bsz: 3084/3819/92; 15823/19594 tok/s;   4585 sec;\n",
            "[2024-01-24 01:39:59,343 INFO] Step 5600/10000; acc: 74.2; ppl:  11.6; xent: 2.5; lr: 0.00118; sents:   72699; bsz: 3111/3799/91; 15935/19461 tok/s;   4741 sec;\n",
            "[2024-01-24 01:40:56,982 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1993)\n",
            "\n",
            "[2024-01-24 01:40:56,982 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-01-24 01:43:21,419 INFO] Step 5800/10000; acc: 74.9; ppl:  11.1; xent: 2.4; lr: 0.00116; sents:   74435; bsz: 3118/3826/93; 12343/15146 tok/s;   4943 sec;\n",
            "[2024-01-24 01:45:57,292 INFO] Step 6000/10000; acc: 75.1; ppl:  11.0; xent: 2.4; lr: 0.00114; sents:   74061; bsz: 3098/3799/93; 15901/19499 tok/s;   5099 sec;\n",
            "[2024-01-24 01:45:57,464 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11)\n",
            "\n",
            "[2024-01-24 01:46:00,809 INFO] valid stats calculation\n",
            "                           took: 3.514622449874878 s.\n",
            "[2024-01-24 01:46:00,811 INFO] Train perplexity: 25.0245\n",
            "[2024-01-24 01:46:00,811 INFO] Train accuracy: 62.0175\n",
            "[2024-01-24 01:46:00,811 INFO] Sentences processed: 2.20782e+06\n",
            "[2024-01-24 01:46:00,811 INFO] Average bsz: 3107/3816/92\n",
            "[2024-01-24 01:46:00,811 INFO] Validation perplexity: 12.5118\n",
            "[2024-01-24 01:46:00,811 INFO] Validation accuracy: 73.7235\n",
            "[2024-01-24 01:46:00,812 INFO] Model is improving ppl: 13.7608 --> 12.5118.\n",
            "[2024-01-24 01:46:00,812 INFO] Model is improving acc: 71.7919 --> 73.7235.\n",
            "[2024-01-24 01:46:00,837 INFO] Saving checkpoint models/model-base.enfr_step_6000.pt\n",
            "[2024-01-24 01:49:04,328 INFO] Step 6200/10000; acc: 74.9; ppl:  11.1; xent: 2.4; lr: 0.00112; sents:   72425; bsz: 3099/3834/91; 13254/16400 tok/s;   5286 sec;\n",
            "[2024-01-24 01:51:35,883 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=989)\n",
            "\n",
            "[2024-01-24 01:51:35,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-01-24 01:52:25,445 INFO] Step 6400/10000; acc: 75.2; ppl:  10.9; xent: 2.4; lr: 0.00110; sents:   73031; bsz: 3117/3818/91; 12400/15187 tok/s;   5487 sec;\n",
            "[2024-01-24 01:54:58,080 INFO] Step 6600/10000; acc: 76.2; ppl:  10.3; xent: 2.3; lr: 0.00109; sents:   71998; bsz: 3101/3828/90; 16254/20063 tok/s;   5640 sec;\n",
            "[2024-01-24 01:57:32,795 INFO] Step 6800/10000; acc: 75.9; ppl:  10.5; xent: 2.4; lr: 0.00107; sents:   74558; bsz: 3119/3808/93; 16130/19690 tok/s;   5794 sec;\n",
            "[2024-01-24 02:00:07,330 INFO] Step 7000/10000; acc: 75.9; ppl:  10.5; xent: 2.4; lr: 0.00106; sents:   71911; bsz: 3100/3810/90; 16050/19724 tok/s;   5949 sec;\n",
            "[2024-01-24 02:03:25,686 INFO] Step 7200/10000; acc: 76.4; ppl:  10.3; xent: 2.3; lr: 0.00104; sents:   76625; bsz: 3120/3817/96; 12585/15396 tok/s;   6147 sec;\n",
            "[2024-01-24 02:05:58,515 INFO] Step 7400/10000; acc: 76.7; ppl:  10.1; xent: 2.3; lr: 0.00103; sents:   74722; bsz: 3084/3810/93; 16142/19944 tok/s;   6300 sec;\n",
            "[2024-01-24 02:08:32,647 INFO] Step 7600/10000; acc: 76.6; ppl:  10.1; xent: 2.3; lr: 0.00101; sents:   72261; bsz: 3116/3831/90; 16176/19883 tok/s;   6454 sec;\n",
            "[2024-01-24 02:11:11,431 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2036)\n",
            "\n",
            "[2024-01-24 02:11:11,431 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-01-24 02:11:45,853 INFO] Step 7800/10000; acc: 76.5; ppl:  10.2; xent: 2.3; lr: 0.00100; sents:   72762; bsz: 3112/3814/91; 12885/15793 tok/s;   6647 sec;\n",
            "[2024-01-24 02:14:18,341 INFO] Step 8000/10000; acc: 77.8; ppl:   9.6; xent: 2.3; lr: 0.00099; sents:   76993; bsz: 3132/3828/96; 16434/20085 tok/s;   6800 sec;\n",
            "[2024-01-24 02:14:18,506 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11)\n",
            "\n",
            "[2024-01-24 02:14:25,750 INFO] valid stats calculation\n",
            "                           took: 7.407334804534912 s.\n",
            "[2024-01-24 02:14:25,752 INFO] Train perplexity: 20.0696\n",
            "[2024-01-24 02:14:25,752 INFO] Train accuracy: 65.5706\n",
            "[2024-01-24 02:14:25,752 INFO] Sentences processed: 2.9451e+06\n",
            "[2024-01-24 02:14:25,752 INFO] Average bsz: 3108/3817/92\n",
            "[2024-01-24 02:14:25,752 INFO] Validation perplexity: 12.0232\n",
            "[2024-01-24 02:14:25,752 INFO] Validation accuracy: 74.6048\n",
            "[2024-01-24 02:14:25,752 INFO] Model is improving ppl: 12.5118 --> 12.0232.\n",
            "[2024-01-24 02:14:25,752 INFO] Model is improving acc: 73.7235 --> 74.6048.\n",
            "[2024-01-24 02:14:25,768 INFO] Saving checkpoint models/model-base.enfr_step_8000.pt\n",
            "[2024-01-24 02:17:20,788 INFO] Step 8200/10000; acc: 77.4; ppl:   9.8; xent: 2.3; lr: 0.00098; sents:   73123; bsz: 3090/3813/91; 13548/16720 tok/s;   6982 sec;\n",
            "[2024-01-24 02:19:54,950 INFO] Step 8400/10000; acc: 77.1; ppl:   9.9; xent: 2.3; lr: 0.00096; sents:   70626; bsz: 3095/3811/88; 16063/19779 tok/s;   7137 sec;\n",
            "[2024-01-24 02:21:37,288 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=983)\n",
            "\n",
            "[2024-01-24 02:21:37,289 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-01-24 02:23:14,306 INFO] Step 8600/10000; acc: 77.7; ppl:   9.6; xent: 2.3; lr: 0.00095; sents:   73055; bsz: 3125/3819/91; 12542/15324 tok/s;   7336 sec;\n",
            "[2024-01-24 02:25:47,410 INFO] Step 8800/10000; acc: 78.2; ppl:   9.4; xent: 2.2; lr: 0.00094; sents:   74370; bsz: 3103/3815/93; 16213/19935 tok/s;   7489 sec;\n",
            "[2024-01-24 02:28:21,877 INFO] Step 9000/10000; acc: 78.0; ppl:   9.5; xent: 2.3; lr: 0.00093; sents:   74539; bsz: 3102/3810/93; 16068/19731 tok/s;   7643 sec;\n",
            "[2024-01-24 02:30:56,623 INFO] Step 9200/10000; acc: 77.6; ppl:   9.6; xent: 2.3; lr: 0.00092; sents:   72176; bsz: 3106/3819/90; 16055/19746 tok/s;   7798 sec;\n",
            "[2024-01-24 02:34:15,226 INFO] Step 9400/10000; acc: 78.8; ppl:   9.1; xent: 2.2; lr: 0.00091; sents:   72498; bsz: 3128/3817/91; 12599/15377 tok/s;   7997 sec;\n",
            "[2024-01-24 02:36:49,141 INFO] Step 9600/10000; acc: 78.3; ppl:   9.3; xent: 2.2; lr: 0.00090; sents:   72926; bsz: 3088/3821/91; 16049/19861 tok/s;   8151 sec;\n",
            "[2024-01-24 02:39:24,071 INFO] Step 9800/10000; acc: 78.5; ppl:   9.2; xent: 2.2; lr: 0.00089; sents:   74752; bsz: 3112/3829/93; 16067/19773 tok/s;   8306 sec;\n",
            "[2024-01-24 02:41:13,214 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=2004)\n",
            "\n",
            "[2024-01-24 02:41:13,215 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-01-24 02:42:41,989 INFO] Step 10000/10000; acc: 78.8; ppl:   9.1; xent: 2.2; lr: 0.00088; sents:   74013; bsz: 3120/3829/93; 12611/15476 tok/s;   8504 sec;\n",
            "[2024-01-24 02:42:42,158 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=11)\n",
            "\n",
            "[2024-01-24 02:42:45,395 INFO] valid stats calculation\n",
            "                           took: 3.4037258625030518 s.\n",
            "[2024-01-24 02:42:45,397 INFO] Train perplexity: 17.2653\n",
            "[2024-01-24 02:42:45,397 INFO] Train accuracy: 68.0642\n",
            "[2024-01-24 02:42:45,397 INFO] Sentences processed: 3.67718e+06\n",
            "[2024-01-24 02:42:45,397 INFO] Average bsz: 3108/3817/92\n",
            "[2024-01-24 02:42:45,397 INFO] Validation perplexity: 11.937\n",
            "[2024-01-24 02:42:45,397 INFO] Validation accuracy: 74.971\n",
            "[2024-01-24 02:42:45,397 INFO] Model is improving ppl: 12.0232 --> 11.937.\n",
            "[2024-01-24 02:42:45,397 INFO] Model is improving acc: 74.6048 --> 74.971.\n",
            "[2024-01-24 02:42:45,413 INFO] Saving checkpoint models/model-base.enfr_step_10000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case Colab is suddenly unable to navigate through directories\n",
        "import os\n",
        "path = '/content/drive/MyDrive/domain-adapted-nmt/nmt-tools'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "qeAMnd2X2uuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -gpu 0 to use gpu\n",
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-general.en-filtered.en.subword.test -output general-fr.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "cdKHqwOJ1kKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba269d97-f70d-430d-bf32-3f60f99f6eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 02:44:04.733112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 02:44:04.733175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 02:44:04.735231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 02:44:04.767921: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 02:44:06.891483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 02:44:08.600018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 02:44:08.600428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 02:44:08.600644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 02:44:09,016 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-24 02:44:12,028 INFO] Loading data into the model\n",
            "[2024-01-24 02:45:42,796 INFO] PRED SCORE: -0.3127, PRED PPL: 1.37 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  93.84438824653625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-technology.en-filtered.en.subword.test -output general-fr-tech.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5747a3df-24b5-414e-904d-db6d341897e3",
        "id": "PrJaqyyG_rVI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 05:48:01.811559: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:48:01.811612: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:48:01.813467: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:48:01.839392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 05:48:05.186479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 05:48:06.509391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:48:06.509880: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:48:06.510084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 05:48:07,365 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-24 05:48:30,898 INFO] Loading data into the model\n",
            "[2024-01-24 05:50:46,932 INFO] PRED SCORE: -0.7975, PRED PPL: 2.22 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  159.62607669830322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-medicine.en-filtered.en.subword.test -output general-fr-medicine.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a7fca5-d599-4578-a688-6140a21f327c",
        "id": "BDx0ml_c_vUA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 05:50:52.205677: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 05:50:52.205743: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 05:50:52.207167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 05:50:52.215063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 05:50:53.433015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 05:50:54.543420: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:50:54.543899: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 05:50:54.544075: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 05:50:54,970 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-24 05:50:57,863 INFO] Loading data into the model\n",
            "[2024-01-24 05:52:27,786 INFO] PRED SCORE: -0.6990, PRED PPL: 2.01 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  92.87819266319275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-fr.translated"
      ],
      "metadata": {
        "id": "FVwRgj_O1nXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ebbb61-c730-4293-e8a0-0755b5bfc72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁Il ▁ne ▁faut ▁pas ▁oublier ▁que ▁ 1 ▁ 2 6 0 ▁prisonniers ▁marocains ▁sont ▁toujours ▁détenus ▁dans ▁des ▁prisons ▁du ▁Front ▁POLISARIO , ▁où ▁ils ▁ont ▁été ▁maintenus ▁pendant ▁plus ▁de ▁ 2 5 ▁ans ▁en ▁violation ▁flagrante ▁du ▁droit ▁international ▁humanitaire .\n",
            "▁Fournir ▁au ▁Comité ▁des ▁exemplaires ▁du ▁texte ▁de ▁la ▁Convention ▁relative ▁aux ▁droits ▁de ▁l ' enfant ▁dans ▁toutes ▁les ▁langues ▁officielles ▁ou ▁dans ▁d ' autres ▁langues ▁ou ▁dialectes , ▁lorsque ▁cela ▁est ▁disponible .\n",
            "▁Nous ▁réaffirmons ▁que ▁les ▁lois ▁en ▁vigueur ▁dans ▁le ▁Sultanat ▁garantissent ▁la ▁protection ▁des ▁droits ▁de ▁l ' homme , ▁y ▁compris ▁les ▁droits ▁de ▁l ' enfant , ▁en ▁particulier ▁en ▁ce ▁qui ▁concerne ▁l ' implication ▁d ' enfants ▁dans ▁les ▁conflits ▁armés .\n",
            "▁Conformément ▁à ▁l ' accord ▁auquel ▁le ▁Conseil ▁est ▁parvenu ▁lors ▁de ▁ses ▁consultations ▁préalables , ▁j ' invite ▁les ▁membres ▁du ▁Conseil ▁à ▁poursuivre ▁l ' examen ▁de ▁la ▁question .\n",
            "▁Décisions ▁et ▁recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q sentencepiece"
      ],
      "metadata": {
        "id": "f76FH1TE1quR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c572e09f-e4dc-4a93-a63a-1cec854738fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model general-fr.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0e2k30QWshy",
        "outputId": "dd576ffb-19b0-40c2-b1d6-8b482844c005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: general-fr.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-technology.model general-fr-tech.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-medicine.model general-fr-medicine.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7ac451-5c7a-44c0-a92b-9f31ca5ce2b3",
        "id": "-sWyEHJd_1Om"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: general-fr-tech.translated.desubword\n",
            "Done desubwording! Output: general-fr-medicine.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-fr.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXEr-jHqWqXN",
        "outputId": "ac1c66be-7c53-427e-9405-fcfdafab281a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours détenus dans des prisons du Front POLISARIO, où ils ont été maintenus pendant plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "Fournir au Comité des exemplaires du texte de la Convention relative aux droits de l'enfant dans toutes les langues officielles ou dans d'autres langues ou dialectes, lorsque cela est disponible.\n",
            "Nous réaffirmons que les lois en vigueur dans le Sultanat garantissent la protection des droits de l'homme, y compris les droits de l'enfant, en particulier en ce qui concerne l'implication d'enfants dans les conflits armés.\n",
            "Conformément à l'accord auquel le Conseil est parvenu lors de ses consultations préalables, j'invite les membres du Conseil à poursuivre l'examen de la question.\n",
            "Décisions et recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model corpora/enfr/en-fr-general.fr-filtered.fr.subword.test"
      ],
      "metadata": {
        "id": "E_1RhBhL1tZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2167c3df-9e7c-46a2-a775-9697e1488929"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-technology.model corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-medicine.model corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43f595d-19db-488e-aff2-4b9511563eab",
        "id": "Hy4p6uNsARgB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword\n",
            "Done desubwording! Output: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword"
      ],
      "metadata": {
        "id": "ZG3qa1AX_o9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1c5268-f355-4fea-a113-2e66c078cfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "Faire parvenir au Comité des exemplaires du texte de la Convention relative aux droits de l'enfant dans toutes les langues officielles de l'État partie ainsi que dans ses autres langues ou dialectes, si elle a été traduite.\n",
            "Nous réaffirmons que la législation en vigueur dans le Sultanat garantit la protection des droits de l'homme, y compris les droits de l'enfant, en particulier pour ce qui est de l'implication d'enfants dans les conflits armés.\n",
            "Conformément à l'accord auquel le Conseil est parvenu lors de ses consultations préalables, j'invite à présent les membres du Conseil à poursuivre le débat sur la question dans le cadre de consultations.\n",
            "Décisions et recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test bleu for baseline score\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ],
      "metadata": {
        "id": "T5bw8Qqp1uzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ee8f69-1686-4430-f5cd-cbe139ebce4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-24 02:43:49--  https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 957 [text/plain]\n",
            "Saving to: ‘compute-bleu.py’\n",
            "\n",
            "compute-bleu.py     100%[===================>]     957  --.-KB/s    in 0s      \n",
            "\n",
            "2024-01-24 02:43:49 (4.60 MB/s) - ‘compute-bleu.py’ saved [957/957]\n",
            "\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.23.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 compute-bleu.py corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword general-fr.translated.desubword"
      ],
      "metadata": {
        "id": "ZidBkQGF5xAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b0ae7a-805c-46f9-ed34-252d32aa7fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "MTed 1st sentence: Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours détenus dans des prisons du Front POLISARIO, où ils ont été maintenus pendant plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "BLEU:  43.0993809551098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 compute-bleu.py corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword general-fr-tech.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword general-fr-medicine.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7796dce-1b26-4c2e-8b70-2cbe2114ec7a",
        "id": "UeOC3dz9AYER"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Crée une nouvelle demande de réunion\n",
            "MTed 1st sentence: Créer une nouvelle demande de réunion\n",
            "BLEU:  8.215118045246678\n",
            "Reference 1st sentence: Cependant, le temps nécessaire à l’ aggravation de la maladie après le traitement était le même dans les deux groupes (environ 10 mois).\n",
            "MTed 1st sentence: Toutefois, le temps écoulé avant que la maladie ne soit▁pire après le traitement a été le même dans les deux groupes (environ 10 mois).\n",
            "BLEU:  11.436729375697029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU scores for \"out-of-domain\" text, or in this case, non-generic text, are quite horrible. This is expected since the baseline model hasn't seen any tech or med-specific inputs.\n",
        "\n",
        "Later on we can fine-tune this model with more generic corpora to improve it even more, but first we'll fine-tune our domain-adapted models for technology and medicine."
      ],
      "metadata": {
        "id": "oxuqKNlUEzpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for technology corpus"
      ],
      "metadata": {
        "id": "imMXZvK6DRIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tech = \"\"\"# tech.yaml\n",
        "# for technology corpus/model fine-tuning\n",
        "share_vocab: true\n",
        "src_vocab: run-tech/source.vocab\n",
        "tgt_vocab: run-tech/target.vocab\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "data:\n",
        "  # different corpus weighting for mixed fine-tuning approach\n",
        "  tech_corpus:\n",
        "    path_src: corpora/enfr/en-fr-technology.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 10\n",
        "  # randomly sample portions from the generic/general data we used to train the baseline model for mixed fine-tuning\n",
        "  gen_corpus:\n",
        "    path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 1\n",
        "  valid: # validation set\n",
        "    path_src: corpora/enfr/en-fr-technology.en-filtered.en.subword.dev\n",
        "    path_tgt: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.dev\n",
        "    transforms: [filtertoolong]\n",
        "\n",
        "update_vocab: true\n",
        "train_from: 'models/model-base.enfr_step_10000.pt' # the base model trained earlier\n",
        "reset_optim: all\n",
        "\n",
        "# filtertoolong\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# tokenization\n",
        "src_subword_model: source-technology.model\n",
        "tgt_subword_model: target-technology.model\n",
        "\n",
        "save_data: run-tech\n",
        "save_model: models/model-tech.enfr\n",
        "log_file: train-tech.log\n",
        "early_stopping: 4\n",
        "\n",
        "keep_checkpoint: 4\n",
        "save_checkpoint_steps: 1000\n",
        "average_decay: 0.0005\n",
        "seed: 1234\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "train_steps: 3000\n",
        "valid_steps: 1000\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"tech.yaml\", \"w+\") as tech_yaml:\n",
        "  tech_yaml.write(tech)"
      ],
      "metadata": {
        "id": "05xpXlOBwjAJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_build_vocab -config tech.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fze04KECkIJ",
        "outputId": "69059dcc-edde-4a45-c2a4-55e554ca8e7c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 15:39:18.313429: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 15:39:18.313496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 15:39:18.315708: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 15:39:18.328276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 15:39:20.784695: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 15:39:22.285064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 15:39:22.285558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 15:39:22.285739: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 15:39:23,486 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-24 15:39:23,486 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-24 15:39:26,152 INFO] * Transform statistics for tech_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=38)\n",
            "\n",
            "[2024-01-24 15:39:26,175 INFO] * Transform statistics for tech_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=35)\n",
            "\n",
            "[2024-01-24 15:40:00,092 INFO] * Transform statistics for gen_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1550)\n",
            "\n",
            "[2024-01-24 15:40:00,373 INFO] * Transform statistics for gen_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1589)\n",
            "\n",
            "[2024-01-24 15:40:00,551 INFO] Counters src: 55902\n",
            "[2024-01-24 15:40:00,551 INFO] Counters tgt: 57951\n",
            "[2024-01-24 15:40:00,593 INFO] Counters after share:95950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_train -config tech.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq4QmVfZCmEH",
        "outputId": "3f7c96e3-9852-4b67-ed61-ecde89b31387"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 15:40:08.147874: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 15:40:08.147942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 15:40:08.153808: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 15:40:08.188451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 15:40:11.627788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 15:40:14.193752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 15:40:14.194287: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 15:40:14.194511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 15:40:15,045 INFO] Parsed 3 corpora from -data.\n",
            "[2024-01-24 15:40:15,259 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-24 15:40:30,651 INFO] Updating checkpoint vocabulary with new vocabulary\n",
            "[2024-01-24 15:40:30,652 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-24 15:40:30,920 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', ',', '▁the', '.', '▁de', \"'\", '▁']\n",
            "[2024-01-24 15:40:30,922 INFO] The decoder start token is: <s>\n",
            "[2024-01-24 15:40:30,947 INFO] Building model...\n",
            "[2024-01-24 15:40:32,745 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-24 15:40:32,746 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-24 15:40:32,746 INFO] Updating vocabulary embeddings with checkpoint embeddings\n",
            "[2024-01-24 15:40:33,102 INFO] src: 21343 new tokens\n",
            "[2024-01-24 15:40:34,271 INFO] tgt: 17895 new tokens\n",
            "[2024-01-24 15:40:34,982 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=50000, bias=True)\n",
            ")\n",
            "[2024-01-24 15:40:34,989 INFO] encoder: 44487680\n",
            "[2024-01-24 15:40:34,989 INFO] decoder: 76435280\n",
            "[2024-01-24 15:40:34,990 INFO] * number of parameters: 120922960\n",
            "[2024-01-24 15:40:34,991 INFO] Trainable parameters = {'torch.float32': 120922960, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-24 15:40:34,992 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-24 15:40:34,992 INFO]  * src vocab size = 50000\n",
            "[2024-01-24 15:40:34,992 INFO]  * tgt vocab size = 50000\n",
            "[2024-01-24 15:40:36,077 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 1\n",
            "[2024-01-24 15:40:36,077 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 1\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:40:36,077 INFO] Starting training on GPU: [0]\n",
            "[2024-01-24 15:40:36,077 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-01-24 15:40:36,078 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=150))\n",
            "[2024-01-24 15:40:38,114 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 2\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:40:40,328 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 3\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:40:41,641 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 4\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:40:43,567 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 5\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:40:45,662 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 6\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:42:20,681 INFO] Step 100/ 3000; acc: 58.6; ppl:  47.7; xent: 3.9; lr: 0.00028; sents:   97773; bsz: 2618/3429/244; 10012/13111 tok/s;    105 sec;\n",
            "[2024-01-24 15:43:06,216 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=388)\n",
            "\n",
            "[2024-01-24 15:43:06,217 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 7\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:43:10,870 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 8\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:43:12,987 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 9\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:43:17,913 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 10\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:43:19,231 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 11\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:44:14,976 INFO] Step 200/ 3000; acc: 67.2; ppl:  20.3; xent: 3.0; lr: 0.00056; sents:   97988; bsz: 2600/3419/245; 9098/11965 tok/s;    219 sec;\n",
            "[2024-01-24 15:45:38,297 INFO] Step 300/ 3000; acc: 73.1; ppl:  13.8; xent: 2.6; lr: 0.00084; sents:  100758; bsz: 2618/3426/252; 12568/16446 tok/s;    302 sec;\n",
            "[2024-01-24 15:45:44,645 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-24 15:45:44,646 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 12\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:45:46,593 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 13\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:45:48,476 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 14\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:45:53,203 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 15\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:45:54,369 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 16\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:47:31,897 INFO] Step 400/ 3000; acc: 77.1; ppl:  10.8; xent: 2.4; lr: 0.00112; sents:   95049; bsz: 2588/3425/238; 9111/12059 tok/s;    416 sec;\n",
            "[2024-01-24 15:48:22,526 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-24 15:48:22,526 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 17\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:48:23,985 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 18\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:48:25,219 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 19\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:48:30,358 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 20\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:48:31,560 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 21\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:48:32,737 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 22\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:49:26,379 INFO] Step 500/ 3000; acc: 78.5; ppl:  10.0; xent: 2.3; lr: 0.00140; sents:   96099; bsz: 2630/3404/240; 9189/11895 tok/s;    530 sec;\n",
            "[2024-01-24 15:50:50,935 INFO] Step 600/ 3000; acc: 79.9; ppl:   9.4; xent: 2.2; lr: 0.00168; sents:  104365; bsz: 2619/3457/261; 12387/16352 tok/s;    615 sec;\n",
            "[2024-01-24 15:51:03,746 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=383)\n",
            "\n",
            "[2024-01-24 15:51:03,746 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 23\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:51:05,058 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 24\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:51:10,759 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 25\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:51:12,154 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 26\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:51:13,362 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 27\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:52:45,923 INFO] Step 700/ 3000; acc: 80.3; ppl:   9.1; xent: 2.2; lr: 0.00196; sents:   93116; bsz: 2576/3415/233; 8961/11880 tok/s;    730 sec;\n",
            "[2024-01-24 15:53:43,666 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-24 15:53:43,666 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 28\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:53:45,780 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 29\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:53:47,457 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 30\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:53:52,903 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 31\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:53:54,141 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 32\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:54:41,000 INFO] Step 800/ 3000; acc: 80.5; ppl:   9.1; xent: 2.2; lr: 0.00224; sents:  102413; bsz: 2665/3449/256; 9263/11989 tok/s;    845 sec;\n",
            "[2024-01-24 15:56:04,635 INFO] Step 900/ 3000; acc: 80.8; ppl:   9.0; xent: 2.2; lr: 0.00252; sents:  100314; bsz: 2624/3441/251; 12549/16457 tok/s;    929 sec;\n",
            "[2024-01-24 15:56:23,256 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-24 15:56:23,256 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 33\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:56:24,555 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 34\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:56:29,868 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 35\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:56:31,140 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 36\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:56:32,963 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 37\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:58:00,579 INFO] Step 1000/ 3000; acc: 79.8; ppl:   9.4; xent: 2.2; lr: 0.00279; sents:   98528; bsz: 2551/3381/246; 8801/11663 tok/s;   1045 sec;\n",
            "[2024-01-24 15:58:02,958 INFO] valid stats calculation\n",
            "                           took: 2.376793146133423 s.\n",
            "[2024-01-24 15:58:02,963 INFO] Train perplexity: 12.5248\n",
            "[2024-01-24 15:58:02,963 INFO] Train accuracy: 75.5834\n",
            "[2024-01-24 15:58:02,963 INFO] Sentences processed: 986403\n",
            "[2024-01-24 15:58:02,963 INFO] Average bsz: 2609/3424/247\n",
            "[2024-01-24 15:58:02,963 INFO] Validation perplexity: 11.4196\n",
            "[2024-01-24 15:58:02,963 INFO] Validation accuracy: 78.8076\n",
            "[2024-01-24 15:58:02,963 INFO] Model is improving ppl: inf --> 11.4196.\n",
            "[2024-01-24 15:58:02,963 INFO] Model is improving acc: -inf --> 78.8076.\n",
            "[2024-01-24 15:58:03,003 INFO] Saving checkpoint models/model-tech.enfr_step_1000.pt\n",
            "[2024-01-24 15:59:25,175 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-24 15:59:25,176 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 38\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:59:26,459 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 39\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:59:31,542 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 40\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:59:32,778 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 41\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:59:39,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 42\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 15:59:40,312 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 43\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:00:20,253 INFO] Step 1100/ 3000; acc: 80.1; ppl:   9.3; xent: 2.2; lr: 0.00266; sents:   95778; bsz: 2644/3422/239; 7571/9801 tok/s;   1184 sec;\n",
            "[2024-01-24 16:01:43,637 INFO] Step 1200/ 3000; acc: 81.0; ppl:   9.0; xent: 2.2; lr: 0.00255; sents:   99014; bsz: 2588/3427/248; 12416/16441 tok/s;   1268 sec;\n",
            "[2024-01-24 16:02:10,490 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=388)\n",
            "\n",
            "[2024-01-24 16:02:10,491 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 44\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:02:12,294 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 45\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:02:13,592 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 46\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:02:19,234 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 47\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:02:20,523 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 48\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:03:40,048 INFO] Step 1300/ 3000; acc: 81.0; ppl:   9.0; xent: 2.2; lr: 0.00245; sents:   94085; bsz: 2597/3405/235; 8925/11700 tok/s;   1384 sec;\n",
            "[2024-01-24 16:04:52,690 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=386)\n",
            "\n",
            "[2024-01-24 16:04:52,691 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 49\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:04:54,070 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 50\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:05:01,072 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 51\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:05:02,420 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 52\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:05:03,742 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 53\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:05:43,819 INFO] Step 1400/ 3000; acc: 82.4; ppl:   8.5; xent: 2.1; lr: 0.00236; sents:  102335; bsz: 2610/3426/256; 8436/11072 tok/s;   1508 sec;\n",
            "[2024-01-24 16:07:07,669 INFO] Step 1500/ 3000; acc: 82.4; ppl:   8.5; xent: 2.1; lr: 0.00228; sents:   96760; bsz: 2608/3434/242; 12442/16379 tok/s;   1592 sec;\n",
            "[2024-01-24 16:07:39,890 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-24 16:07:39,891 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 54\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:07:41,253 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 55\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:07:46,510 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 56\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:07:47,993 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 57\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:07:54,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 58\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:07:56,103 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 59\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:09:03,494 INFO] Step 1600/ 3000; acc: 83.4; ppl:   8.2; xent: 2.1; lr: 0.00221; sents:   99461; bsz: 2614/3420/249; 9028/11811 tok/s;   1707 sec;\n",
            "[2024-01-24 16:10:25,307 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=383)\n",
            "\n",
            "[2024-01-24 16:10:25,308 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 60\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:10:26,617 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 61\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:10:32,229 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 62\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:10:33,473 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 63\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:10:34,716 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 64\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:11:03,051 INFO] Step 1700/ 3000; acc: 83.0; ppl:   8.3; xent: 2.1; lr: 0.00214; sents:  100004; bsz: 2579/3393/250; 8628/11351 tok/s;   1827 sec;\n",
            "[2024-01-24 16:12:25,982 INFO] Step 1800/ 3000; acc: 83.3; ppl:   8.2; xent: 2.1; lr: 0.00208; sents:   97812; bsz: 2596/3427/245; 12522/16531 tok/s;   1910 sec;\n",
            "[2024-01-24 16:13:05,583 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-24 16:13:05,583 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 65\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:13:06,880 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 66\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:13:12,846 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 67\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:13:14,074 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 68\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:13:15,293 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 69\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:14:19,621 INFO] Step 1900/ 3000; acc: 82.9; ppl:   8.4; xent: 2.1; lr: 0.00203; sents:   89776; bsz: 2604/3372/224; 9164/11871 tok/s;   2024 sec;\n",
            "[2024-01-24 16:15:43,109 INFO] Step 2000/ 3000; acc: 84.4; ppl:   7.9; xent: 2.1; lr: 0.00198; sents:  103136; bsz: 2605/3431/258; 12480/16437 tok/s;   2107 sec;\n",
            "[2024-01-24 16:15:43,186 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=5)\n",
            "\n",
            "[2024-01-24 16:15:44,772 INFO] valid stats calculation\n",
            "                           took: 1.6605439186096191 s.\n",
            "[2024-01-24 16:15:44,775 INFO] Train perplexity: 10.3323\n",
            "[2024-01-24 16:15:44,775 INFO] Train accuracy: 78.9804\n",
            "[2024-01-24 16:15:44,775 INFO] Sentences processed: 1.96456e+06\n",
            "[2024-01-24 16:15:44,775 INFO] Average bsz: 2607/3420/246\n",
            "[2024-01-24 16:15:44,775 INFO] Validation perplexity: 10.6574\n",
            "[2024-01-24 16:15:44,775 INFO] Validation accuracy: 81.0742\n",
            "[2024-01-24 16:15:44,775 INFO] Model is improving ppl: 11.4196 --> 10.6574.\n",
            "[2024-01-24 16:15:44,775 INFO] Model is improving acc: 78.8076 --> 81.0742.\n",
            "[2024-01-24 16:15:44,796 INFO] Saving checkpoint models/model-tech.enfr_step_2000.pt\n",
            "[2024-01-24 16:15:55,965 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=389)\n",
            "\n",
            "[2024-01-24 16:15:55,966 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 70\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:15:57,251 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 71\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:16:03,368 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 72\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:16:05,161 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 73\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:16:08,001 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 74\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:17:55,269 INFO] Step 2100/ 3000; acc: 84.4; ppl:   7.9; xent: 2.1; lr: 0.00193; sents:  100144; bsz: 2601/3432/250; 7871/10387 tok/s;   2239 sec;\n",
            "[2024-01-24 16:18:40,986 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=384)\n",
            "\n",
            "[2024-01-24 16:18:40,987 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 75\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:18:42,615 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 76\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:18:48,000 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 77\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:18:49,262 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 78\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:18:50,539 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 79\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:18:57,012 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 80\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:19:50,705 INFO] Step 2200/ 3000; acc: 84.6; ppl:   7.8; xent: 2.1; lr: 0.00188; sents:  102139; bsz: 2617/3429/255; 9068/11881 tok/s;   2355 sec;\n",
            "[2024-01-24 16:21:12,752 INFO] Step 2300/ 3000; acc: 84.1; ppl:   8.0; xent: 2.1; lr: 0.00184; sents:   93546; bsz: 2617/3417/234; 12757/16660 tok/s;   2437 sec;\n",
            "[2024-01-24 16:21:21,153 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=386)\n",
            "\n",
            "[2024-01-24 16:21:21,154 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 81\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:21:26,475 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 82\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:21:27,701 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 83\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:21:32,913 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 84\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:21:34,125 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 85\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:23:08,169 INFO] Step 2400/ 3000; acc: 84.3; ppl:   7.9; xent: 2.1; lr: 0.00180; sents:   98408; bsz: 2589/3406/246; 8974/11806 tok/s;   2552 sec;\n",
            "[2024-01-24 16:24:00,558 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-24 16:24:00,558 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 86\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:24:01,797 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 87\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:24:06,827 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 88\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:24:08,106 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 89\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:24:10,164 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 90\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:25:00,775 INFO] Step 2500/ 3000; acc: 85.3; ppl:   7.6; xent: 2.0; lr: 0.00177; sents:   97730; bsz: 2625/3445/244; 9323/12236 tok/s;   2665 sec;\n",
            "[2024-01-24 16:26:23,121 INFO] Step 2600/ 3000; acc: 84.7; ppl:   7.8; xent: 2.0; lr: 0.00173; sents:   99087; bsz: 2616/3438/248; 12707/16700 tok/s;   2747 sec;\n",
            "[2024-01-24 16:26:36,590 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-24 16:26:36,591 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 91\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:26:37,809 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 92\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:26:43,064 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 93\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:26:44,302 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 94\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:26:45,491 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 95\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-24 16:26:45,614 INFO] * Transform statistics for gen_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3001)\n",
            "\n",
            "[2024-01-24 16:26:45,614 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 95\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:26:51,009 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 96\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:28:15,616 INFO] Step 2700/ 3000; acc: 84.9; ppl:   7.7; xent: 2.0; lr: 0.00170; sents:   97964; bsz: 2605/3435/245; 9263/12213 tok/s;   2860 sec;\n",
            "[2024-01-24 16:29:14,140 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=381)\n",
            "\n",
            "[2024-01-24 16:29:14,141 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 97\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:29:19,058 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 98\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:29:20,282 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 99\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:29:21,530 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 100\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:29:27,359 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 101\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:30:07,226 INFO] Step 2800/ 3000; acc: 85.3; ppl:   7.6; xent: 2.0; lr: 0.00167; sents:   97037; bsz: 2605/3415/243; 9337/12238 tok/s;   2971 sec;\n",
            "[2024-01-24 16:31:29,876 INFO] Step 2900/ 3000; acc: 85.7; ppl:   7.4; xent: 2.0; lr: 0.00164; sents:   99768; bsz: 2632/3438/249; 12736/16641 tok/s;   3054 sec;\n",
            "[2024-01-24 16:31:53,097 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-24 16:31:53,097 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 102\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:31:54,347 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 103\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:31:56,078 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 104\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:32:01,455 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 105\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:32:02,657 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 106\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-24 16:33:25,373 INFO] Step 3000/ 3000; acc: 85.7; ppl:   7.4; xent: 2.0; lr: 0.00161; sents:  103148; bsz: 2602/3406/258; 9011/11796 tok/s;   3169 sec;\n",
            "[2024-01-24 16:33:25,507 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=5)\n",
            "\n",
            "[2024-01-24 16:33:27,106 INFO] valid stats calculation\n",
            "                           took: 1.728764295578003 s.\n",
            "[2024-01-24 16:33:27,108 INFO] Train perplexity: 9.36857\n",
            "[2024-01-24 16:33:27,108 INFO] Train accuracy: 80.9543\n",
            "[2024-01-24 16:33:27,108 INFO] Sentences processed: 2.95354e+06\n",
            "[2024-01-24 16:33:27,108 INFO] Average bsz: 2608/3422/246\n",
            "[2024-01-24 16:33:27,108 INFO] Validation perplexity: 10.703\n",
            "[2024-01-24 16:33:27,108 INFO] Validation accuracy: 81.6605\n",
            "[2024-01-24 16:33:27,108 INFO] Stalled patience: 3/4\n",
            "[2024-01-24 16:33:27,127 INFO] Saving checkpoint models/model-tech.enfr_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# translate domain-specific and generic corpora\n",
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-technology.en-filtered.en.subword.test -output technology-fr.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXARacPCtZq",
        "outputId": "50b6359c-8e81-4c9a-948e-fbdc5e87d313"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 16:33:45.559234: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 16:33:45.559299: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 16:33:45.560802: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 16:33:45.569309: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 16:33:47.418211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 16:33:49.657154: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:33:49.658268: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:33:49.658698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 16:33:50,851 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-24 16:33:54,897 INFO] Loading data into the model\n",
            "[2024-01-24 16:34:34,466 INFO] PRED SCORE: -0.1746, PRED PPL: 1.19 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  43.65361046791077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-medicine.en-filtered.en.subword.test -output technology-fr-medicine.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsuWq1_SC5w0",
        "outputId": "a09d4ebe-89b0-47c9-9caf-8577ccebb2a7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 16:34:40.648382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 16:34:40.648447: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 16:34:40.650491: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 16:34:40.665115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 16:34:44.368575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 16:34:47.042918: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:34:47.043409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:34:47.043593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 16:34:47,514 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-24 16:34:50,446 INFO] Loading data into the model\n",
            "[2024-01-24 16:35:54,595 INFO] PRED SCORE: -0.4672, PRED PPL: 1.60 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  67.13197183609009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-general.en-filtered.en.subword.test -output technology-fr-general.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4SNaRK-C-gF",
        "outputId": "dd423b32-bdf5-4619-f49c-5af538e23fac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-24 16:35:59.440759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-24 16:35:59.440847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-24 16:35:59.442892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-24 16:35:59.456277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-24 16:36:01.436380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-24 16:36:02.840084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:36:02.840582: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-24 16:36:02.840770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-24 16:36:03,316 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-24 16:36:06,342 INFO] Loading data into the model\n",
            "[2024-01-24 16:37:47,564 INFO] PRED SCORE: -0.3185, PRED PPL: 1.38 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  104.28550958633423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated files\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-technology.model technology-fr.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-medicine.model technology-fr-medicine.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model technology-fr-general.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N5lknruDkMu",
        "outputId": "964cef69-a9d3-44a9-adaa-0d07b0143d0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: technology-fr.translated.desubword\n",
            "Done desubwording! Output: technology-fr-medicine.translated.desubword\n",
            "Done desubwording! Output: technology-fr-general.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU scores for technology-specific NMT model\n",
        "# the filtered test datasets were already desubworded and saved previously when evaluating the baseline model, so we can just reuse them\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword technology-fr.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword technology-fr-medicine.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword technology-fr-general.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udquum9SDymf",
        "outputId": "a5369d8d-995f-4979-a6e0-f57e8f6b4d42"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Crée une nouvelle demande de réunion\n",
            "MTed 1st sentence: Crée une nouvelle demande de réunionNew\n",
            "BLEU:  58.474180647739004\n",
            "Reference 1st sentence: Cependant, le temps nécessaire à l’ aggravation de la maladie après le traitement était le même dans les deux groupes (environ 10 mois).\n",
            "MTed 1st sentence: Toutefois, le temps nécessaire à la maladie est le même pour les deux groupes (environ 10 mois).\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  16.09054285136335\n",
            "Reference 1st sentence: Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "MTed 1st sentence: Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours incarcérés dans des prisons du Front POLISARIO où ils ont été détenus depuis plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "BLEU:  42.52193541201577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for medicine corpus"
      ],
      "metadata": {
        "id": "-HlglKIBEQtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "med = \"\"\"# med.yaml\n",
        "# for medicine corpus/model fine-tuning\n",
        "share_vocab: true\n",
        "src_vocab: run-med/source.vocab\n",
        "tgt_vocab: run-med/target.vocab\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "data:\n",
        "  # different corpus weighting for mixed fine-tuning approach\n",
        "  tech_corpus:\n",
        "    path_src: corpora/enfr/en-fr-medicine.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 10\n",
        "  # randomly sample portions from the generic/general data we used to train the baseline model for mixed fine-tuning\n",
        "  gen_corpus:\n",
        "    path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 1\n",
        "  valid:\n",
        "    path_src: corpora/enfr/en-fr-medicine.en-filtered.en.subword.dev\n",
        "    path_tgt: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.dev\n",
        "    transforms: [filtertoolong]\n",
        "\n",
        "update_vocab: true\n",
        "train_from: 'models/model-base.enfr_step_10000.pt' # the base model trained earlier\n",
        "reset_optim: all\n",
        "\n",
        "# filtertoolong\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# tokenization\n",
        "src_subword_model: source-medicine.model\n",
        "tgt_subword_model: target-medicine.model\n",
        "\n",
        "save_data: run-tech\n",
        "save_model: models/model-med.enfr\n",
        "log_file: train-tech.log\n",
        "early_stopping: 4\n",
        "\n",
        "keep_checkpoint: 4\n",
        "save_checkpoint_steps: 1000\n",
        "average_decay: 0.0005\n",
        "seed: 1234\n",
        "warmup_steps: 2000\n",
        "report_every: 200\n",
        "\n",
        "train_steps: 6000\n",
        "valid_steps: 2000\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"med.yaml\", \"w+\") as med_yaml:\n",
        "  med_yaml.write(med)"
      ],
      "metadata": {
        "id": "t90B79nrErWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_build_vocab -config med.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "cuKVWXT4Er5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_train -config med.yaml"
      ],
      "metadata": {
        "id": "F78AhUXbFXut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}