{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7/JTFa2v35/HG+AYpKj/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cw118/domain-adapted-nmt/blob/main/2_fr_domain_adapted_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A two-step approach to NMT\n",
        "\n",
        "## Part 2: training NMT models"
      ],
      "metadata": {
        "id": "K6TDCS_KZ3UD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eRyWkpHU3-w",
        "outputId": "5ab088e5-01fb-47da-9049-d8853449b22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenNMT-py\n",
            "  Downloading OpenNMT_py-3.4.3-py3-none-any.whl (257 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/257.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.3/257.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<2.2,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.1.0+cu121)\n",
            "Collecting configargparse (from OpenNMT-py)\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Collecting ctranslate2<4,>=3.17 (from OpenNMT-py)\n",
            "  Downloading ctranslate2-3.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.3 in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.15.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (2.2.5)\n",
            "Collecting waitress (from OpenNMT-py)\n",
            "  Downloading waitress-2.1.2-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.35 (from OpenNMT-py)\n",
            "  Downloading pyonmttok-1.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (6.0.1)\n",
            "Collecting sacrebleu (from OpenNMT-py)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz (from OpenNMT-py)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from OpenNMT-py)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext-wheel (from OpenNMT-py)\n",
            "  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (3.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from OpenNMT-py) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ctranslate2<4,>=3.17->OpenNMT-py) (1.23.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.5.2)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.3->OpenNMT-py) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=2.0.1->OpenNMT-py) (2.1.0)\n",
            "Collecting pybind11>=2.2 (from fasttext-wheel->OpenNMT-py)\n",
            "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->OpenNMT-py) (8.1.7)\n",
            "Collecting portalocker (from sacrebleu->OpenNMT-py)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (0.9.0)\n",
            "Collecting colorama (from sacrebleu->OpenNMT-py)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->OpenNMT-py) (4.9.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (4.66.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (1.10.14)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->OpenNMT-py) (3.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2,>=2.0.1->OpenNMT-py) (2.1.4)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy->OpenNMT-py) (0.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3->OpenNMT-py) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->OpenNMT-py) (0.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=2.0.1->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.3->OpenNMT-py) (3.2.2)\n",
            "Installing collected packages: waitress, rapidfuzz, pyonmttok, pybind11, pyahocorasick, portalocker, ctranslate2, configargparse, colorama, sacrebleu, fasttext-wheel, OpenNMT-py\n",
            "Successfully installed OpenNMT-py-3.4.3 colorama-0.4.6 configargparse-1.7 ctranslate2-3.24.0 fasttext-wheel-0.9.2 portalocker-2.8.2 pyahocorasick-2.0.0 pybind11-2.11.1 pyonmttok-1.37.1 rapidfuzz-3.6.1 sacrebleu-2.4.0 waitress-2.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip3 install OpenNMT-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change into folder where prepared datasets were saved in the text processing step\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ex2ZuFaQZ2bc",
        "outputId": "0fbda72f-7610-42b2-f122-aaf0084f5fef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/domain-adapted-nmt/nmt-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVFhh1Fp7mt",
        "outputId": "f6f9a4ad-397b-44ee-f93e-c412714d12d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/domain-adapted-nmt/nmt-tools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the general/base model"
      ],
      "metadata": {
        "id": "Hql2_poQx3ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# corpora generated from step 1: https://drive.google.com/drive/folders/1fVe2e2MvT2CCTpSSDrBkykoR-7JKy-w4?usp=sharing\n",
        "config = '''# config.yaml\n",
        "\n",
        "# where the samples will be written\n",
        "save_data: run\n",
        "\n",
        "# train the general/base model first\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "        path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "        transforms: [filtertoolong]\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.dev\n",
        "        path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.dev\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# vocab files generated by onmt_build_vocab\n",
        "src_vocab: run/source.vocab\n",
        "tgt_vocab: run/target.vocab\n",
        "\n",
        "# vocabulary size: should be same as in sentencepiece\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# Tokenization options\n",
        "src_subword_model: source-general.model\n",
        "tgt_subword_model: target-general.model\n",
        "\n",
        "# Where to save the log file and the output models/checkpoints\n",
        "log_file: train.log\n",
        "save_model: models/model-base.enfr\n",
        "\n",
        "# Stop training if it does not improve after n validations\n",
        "early_stopping: 4\n",
        "\n",
        "# Default: 5000 - Save a model checkpoint for each n\n",
        "save_checkpoint_steps: 2000\n",
        "\n",
        "# To save space, limit checkpoints to last n\n",
        "# keep_checkpoint: 4\n",
        "\n",
        "seed: 3435\n",
        "\n",
        "# For fine-tuning, add up the required steps to the original steps\n",
        "train_steps: 10000\n",
        "\n",
        "# Default: 10000 - Run validation after n steps\n",
        "valid_steps: 2000\n",
        "\n",
        "# Default: 4000 - for large datasets, try up to 8000\n",
        "warmup_steps: 2000\n",
        "report_every: 200\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0  # Default: 2, set to 0 when RAM out of memory\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096   # Tokens per batch, change when CUDA out of memory\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "'''\n",
        "\n",
        "with open(\"config.yaml\", \"w+\") as config_yaml:\n",
        "  config_yaml.write(config)"
      ],
      "metadata": {
        "id": "lZ69S-sHaDfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if this file is already in your Drive, you don't need to run this again\n",
        "!wget https://raw.githubusercontent.com/OpenNMT/OpenNMT-py/master/tools/spm_to_vocab.py"
      ],
      "metadata": {
        "id": "67622tM9gC9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert sentencepiece-generated vocab to be compatible with OpenNMT-py (cf. https://forum.opennmt.net/t/steps-to-convert-sentencepiece-vocab-to-opennmt-py-vocab/4879)\n",
        "!cat source-general.vocab | python3 spm_to_vocab.py > source-general.onmt_vocab"
      ],
      "metadata": {
        "id": "Nca9EgdEfTnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vDUluygfFYy",
        "outputId": "6d486213-1fa9-476a-a2ea-d4c827b54c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# match -num_threads to number of CPUs to increase speed\n",
        "# -1 for -n_sample to use entire corpus when building vocab\n",
        "!onmt_build_vocab -config config.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "SH_-SZuTfI0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332dca8b-db9c-419c-f4b0-cdef5f82db2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-28 05:23:01.582726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 05:23:01.582836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 05:23:01.587370: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-28 05:23:01.604501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-28 05:23:03.179361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[2024-01-28 05:23:08,539 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-28 05:23:08,539 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-28 05:23:52,474 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1589)\n",
            "\n",
            "[2024-01-28 05:23:52,481 INFO] * Transform statistics for corpus_1(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1550)\n",
            "\n",
            "[2024-01-28 05:23:52,763 INFO] Counters src: 49522\n",
            "[2024-01-28 05:23:52,763 INFO] Counters tgt: 49985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# once runtime type is changed to GPU, check that the GPU is active\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do2zzPXFfdeZ",
        "outputId": "ff6910ad-367a-4893-b857-7c0229b56be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-0ed94908-7be9-b88d-005d-e79bdccb6bdf)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check that the GPU is visible to PyTorch\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.get_device_name(0))\n",
        "\n",
        "gpu_memory = torch.cuda.mem_get_info(0)\n",
        "print(\"Free GPU memory:\", gpu_memory[0] / 1024**2, \"out of\", gpu_memory[1] / 1024**2)"
      ],
      "metadata": {
        "id": "8Tz1-U5zgENR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f47676e-7605-48bc-8d8e-f0acf5ea3bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n",
            "Free GPU memory: 14999.0625 out of 15102.0625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the models directory for a fresh start\n",
        "!rm -rf /content/drive/MyDrive/nmt-tools/models"
      ],
      "metadata": {
        "id": "PO3lc2tdgMk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train NMT model\n",
        "!onmt_train -config config.yaml"
      ],
      "metadata": {
        "id": "t8CZzr2XgS24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09556d0-875d-4864-d23f-8bb3f75a4787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-28 15:01:32.174207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 15:01:32.174277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 15:01:32.176258: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-28 15:01:32.187529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-28 15:01:33.637796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-28 15:01:35.130932: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-28 15:01:35.131337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-28 15:01:35.131504: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-28 15:01:36,477 INFO] Parsed 2 corpora from -data.\n",
            "[2024-01-28 15:01:36,686 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-28 15:01:37,449 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', '▁of', ',', '.', '▁and', '▁to']\n",
            "[2024-01-28 15:01:37,449 INFO] The decoder start token is: <s>\n",
            "[2024-01-28 15:01:37,449 INFO] Building model...\n",
            "[2024-01-28 15:01:39,659 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-28 15:01:39,659 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-28 15:01:40,103 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(49528, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(49992, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=49992, bias=True)\n",
            ")\n",
            "[2024-01-28 15:01:40,109 INFO] encoder: 44246016\n",
            "[2024-01-28 15:01:40,109 INFO] decoder: 76427080\n",
            "[2024-01-28 15:01:40,109 INFO] * number of parameters: 120673096\n",
            "[2024-01-28 15:01:40,111 INFO] Trainable parameters = {'torch.float32': 120673096, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-28 15:01:40,111 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-28 15:01:40,111 INFO]  * src vocab size = 49528\n",
            "[2024-01-28 15:01:40,111 INFO]  * tgt vocab size = 49992\n",
            "[2024-01-28 15:01:40,651 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2024-01-28 15:01:40,652 INFO] Starting training on GPU: [0]\n",
            "[2024-01-28 15:01:40,652 INFO] Start training loop and validate every 2000 steps...\n",
            "[2024-01-28 15:01:40,652 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=150))\n",
            "[2024-01-28 15:04:56,499 INFO] Step 200/10000; acc: 7.1; ppl: 4416.2; xent: 8.4; lr: 0.00020; sents:   71619; bsz: 3141/3846/90; 12830/15710 tok/s;    196 sec;\n",
            "[2024-01-28 15:07:32,361 INFO] Step 400/10000; acc: 21.3; ppl: 318.7; xent: 5.8; lr: 0.00040; sents:   76557; bsz: 3141/3839/96; 16124/19705 tok/s;    352 sec;\n",
            "[2024-01-28 15:10:07,928 INFO] Step 600/10000; acc: 29.5; ppl: 148.7; xent: 5.0; lr: 0.00059; sents:   75473; bsz: 3098/3803/94; 15929/19559 tok/s;    507 sec;\n",
            "[2024-01-28 15:11:34,879 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1949)\n",
            "\n",
            "[2024-01-28 15:11:34,880 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2024-01-28 15:13:31,509 INFO] Step 800/10000; acc: 39.1; ppl:  81.1; xent: 4.4; lr: 0.00079; sents:   75447; bsz: 3079/3812/94; 12098/14979 tok/s;    711 sec;\n",
            "[2024-01-28 15:16:08,711 INFO] Step 1000/10000; acc: 47.9; ppl:  50.2; xent: 3.9; lr: 0.00099; sents:   72756; bsz: 3162/3861/91; 16094/19650 tok/s;    868 sec;\n",
            "[2024-01-28 15:18:45,437 INFO] Step 1200/10000; acc: 53.9; ppl:  36.6; xent: 3.6; lr: 0.00119; sents:   77172; bsz: 3100/3822/96; 15826/19508 tok/s;   1025 sec;\n",
            "[2024-01-28 15:22:05,009 INFO] Step 1400/10000; acc: 57.7; ppl:  29.8; xent: 3.4; lr: 0.00138; sents:   73368; bsz: 3126/3834/92; 12531/15370 tok/s;   1224 sec;\n",
            "[2024-01-28 15:24:41,088 INFO] Step 1600/10000; acc: 60.6; ppl:  25.1; xent: 3.2; lr: 0.00158; sents:   74634; bsz: 3099/3829/93; 15885/19626 tok/s;   1380 sec;\n",
            "[2024-01-28 15:27:17,205 INFO] Step 1800/10000; acc: 61.8; ppl:  23.6; xent: 3.2; lr: 0.00178; sents:   73556; bsz: 3111/3820/92; 15939/19573 tok/s;   1537 sec;\n",
            "[2024-01-28 15:29:53,416 INFO] Step 2000/10000; acc: 63.3; ppl:  21.9; xent: 3.1; lr: 0.00198; sents:   75259; bsz: 3138/3842/94; 16070/19674 tok/s;   1693 sec;\n",
            "[2024-01-28 15:29:57,327 INFO] valid stats calculation\n",
            "                           took: 3.909045457839966 s.\n",
            "[2024-01-28 15:29:57,328 INFO] Train perplexity: 81.056\n",
            "[2024-01-28 15:29:57,329 INFO] Train accuracy: 44.2141\n",
            "[2024-01-28 15:29:57,329 INFO] Sentences processed: 745841\n",
            "[2024-01-28 15:29:57,329 INFO] Average bsz: 3119/3831/93\n",
            "[2024-01-28 15:29:57,329 INFO] Validation perplexity: 19.9858\n",
            "[2024-01-28 15:29:57,329 INFO] Validation accuracy: 65.1666\n",
            "[2024-01-28 15:29:57,329 INFO] Model is improving ppl: inf --> 19.9858.\n",
            "[2024-01-28 15:29:57,329 INFO] Model is improving acc: -inf --> 65.1666.\n",
            "[2024-01-28 15:29:57,346 INFO] Saving checkpoint models/model-base.enfr_step_2000.pt\n",
            "[2024-01-28 15:31:22,510 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3903)\n",
            "\n",
            "[2024-01-28 15:31:22,510 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2024-01-28 15:33:30,749 INFO] Step 2200/10000; acc: 65.0; ppl:  19.8; xent: 3.0; lr: 0.00188; sents:   72611; bsz: 3148/3816/91; 11588/14048 tok/s;   1910 sec;\n",
            "[2024-01-28 15:36:06,431 INFO] Step 2400/10000; acc: 66.2; ppl:  18.3; xent: 2.9; lr: 0.00180; sents:   74659; bsz: 3103/3820/93; 15945/19631 tok/s;   2066 sec;\n",
            "[2024-01-28 15:38:42,480 INFO] Step 2600/10000; acc: 67.2; ppl:  17.4; xent: 2.9; lr: 0.00173; sents:   77675; bsz: 3120/3841/97; 15997/19691 tok/s;   2222 sec;\n",
            "[2024-01-28 15:41:25,243 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1926)\n",
            "\n",
            "[2024-01-28 15:41:25,244 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2024-01-28 15:42:04,728 INFO] Step 2800/10000; acc: 68.0; ppl:  16.5; xent: 2.8; lr: 0.00167; sents:   73191; bsz: 3126/3837/91; 12364/15176 tok/s;   2424 sec;\n",
            "[2024-01-28 15:44:40,670 INFO] Step 3000/10000; acc: 69.5; ppl:  15.1; xent: 2.7; lr: 0.00161; sents:   72254; bsz: 3136/3841/90; 16089/19707 tok/s;   2580 sec;\n",
            "[2024-01-28 15:47:16,360 INFO] Step 3200/10000; acc: 69.8; ppl:  14.9; xent: 2.7; lr: 0.00156; sents:   75171; bsz: 3131/3835/94; 16090/19708 tok/s;   2736 sec;\n",
            "[2024-01-28 15:49:52,008 INFO] Step 3400/10000; acc: 70.1; ppl:  14.7; xent: 2.7; lr: 0.00152; sents:   76078; bsz: 3102/3826/95; 15944/19664 tok/s;   2891 sec;\n",
            "[2024-01-28 15:53:13,117 INFO] Step 3600/10000; acc: 71.0; ppl:  13.8; xent: 2.6; lr: 0.00147; sents:   72523; bsz: 3102/3818/91; 12338/15190 tok/s;   3092 sec;\n",
            "[2024-01-28 15:55:48,952 INFO] Step 3800/10000; acc: 71.6; ppl:  13.3; xent: 2.6; lr: 0.00143; sents:   75568; bsz: 3113/3839/94; 15979/19706 tok/s;   3248 sec;\n",
            "[2024-01-28 15:58:25,137 INFO] Step 4000/10000; acc: 71.7; ppl:  13.3; xent: 2.6; lr: 0.00140; sents:   76101; bsz: 3132/3822/95; 16045/19578 tok/s;   3404 sec;\n",
            "[2024-01-28 15:58:25,304 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=24)\n",
            "\n",
            "[2024-01-28 15:58:28,589 INFO] valid stats calculation\n",
            "                           took: 3.450127601623535 s.\n",
            "[2024-01-28 15:58:28,591 INFO] Train perplexity: 35.5373\n",
            "[2024-01-28 15:58:28,591 INFO] Train accuracy: 56.609\n",
            "[2024-01-28 15:58:28,591 INFO] Sentences processed: 1.49167e+06\n",
            "[2024-01-28 15:58:28,591 INFO] Average bsz: 3120/3830/93\n",
            "[2024-01-28 15:58:28,591 INFO] Validation perplexity: 13.6626\n",
            "[2024-01-28 15:58:28,591 INFO] Validation accuracy: 71.8713\n",
            "[2024-01-28 15:58:28,591 INFO] Model is improving ppl: 19.9858 --> 13.6626.\n",
            "[2024-01-28 15:58:28,591 INFO] Model is improving acc: 65.1666 --> 71.8713.\n",
            "[2024-01-28 15:58:28,607 INFO] Saving checkpoint models/model-base.enfr_step_4000.pt\n",
            "[2024-01-28 16:01:29,490 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3880)\n",
            "\n",
            "[2024-01-28 16:01:29,490 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2024-01-28 16:02:20,659 INFO] Step 4200/10000; acc: 71.9; ppl:  13.1; xent: 2.6; lr: 0.00136; sents:   75083; bsz: 3123/3839/94; 10607/13041 tok/s;   3640 sec;\n",
            "[2024-01-28 16:04:57,054 INFO] Step 4400/10000; acc: 73.1; ppl:  12.2; xent: 2.5; lr: 0.00133; sents:   75258; bsz: 3138/3821/94; 16054/19548 tok/s;   3796 sec;\n",
            "[2024-01-28 16:07:32,735 INFO] Step 4600/10000; acc: 72.8; ppl:  12.4; xent: 2.5; lr: 0.00130; sents:   75230; bsz: 3082/3809/94; 15838/19572 tok/s;   3952 sec;\n",
            "[2024-01-28 16:10:08,532 INFO] Step 4800/10000; acc: 73.2; ppl:  12.3; xent: 2.5; lr: 0.00128; sents:   74049; bsz: 3123/3838/93; 16035/19710 tok/s;   4108 sec;\n",
            "[2024-01-28 16:13:29,776 INFO] Step 5000/10000; acc: 73.9; ppl:  11.7; xent: 2.5; lr: 0.00125; sents:   71750; bsz: 3126/3845/90; 12428/15284 tok/s;   4309 sec;\n",
            "[2024-01-28 16:16:05,926 INFO] Step 5200/10000; acc: 74.4; ppl:  11.4; xent: 2.4; lr: 0.00123; sents:   74339; bsz: 3117/3828/93; 15970/19611 tok/s;   4465 sec;\n",
            "[2024-01-28 16:18:41,736 INFO] Step 5400/10000; acc: 74.0; ppl:  11.6; xent: 2.5; lr: 0.00120; sents:   75539; bsz: 3105/3823/94; 15945/19628 tok/s;   4621 sec;\n",
            "[2024-01-28 16:20:58,132 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3874)\n",
            "\n",
            "[2024-01-28 16:20:58,132 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2024-01-28 16:22:08,223 INFO] Step 5600/10000; acc: 74.2; ppl:  11.5; xent: 2.4; lr: 0.00118; sents:   75121; bsz: 3129/3828/94; 12123/14831 tok/s;   4828 sec;\n",
            "[2024-01-28 16:24:44,669 INFO] Step 5800/10000; acc: 75.3; ppl:  10.9; xent: 2.4; lr: 0.00116; sents:   75374; bsz: 3135/3826/94; 16031/19566 tok/s;   4984 sec;\n",
            "[2024-01-28 16:27:20,574 INFO] Step 6000/10000; acc: 75.1; ppl:  11.0; xent: 2.4; lr: 0.00114; sents:   74667; bsz: 3112/3826/93; 15966/19631 tok/s;   5140 sec;\n",
            "[2024-01-28 16:27:20,754 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=24)\n",
            "\n",
            "[2024-01-28 16:27:24,028 INFO] valid stats calculation\n",
            "                           took: 3.451046943664551 s.\n",
            "[2024-01-28 16:27:24,030 INFO] Train perplexity: 24.6072\n",
            "[2024-01-28 16:27:24,030 INFO] Train accuracy: 62.3362\n",
            "[2024-01-28 16:27:24,030 INFO] Sentences processed: 2.23808e+06\n",
            "[2024-01-28 16:27:24,030 INFO] Average bsz: 3120/3830/93\n",
            "[2024-01-28 16:27:24,030 INFO] Validation perplexity: 12.4685\n",
            "[2024-01-28 16:27:24,030 INFO] Validation accuracy: 73.6879\n",
            "[2024-01-28 16:27:24,030 INFO] Model is improving ppl: 13.6626 --> 12.4685.\n",
            "[2024-01-28 16:27:24,030 INFO] Model is improving acc: 71.8713 --> 73.6879.\n",
            "[2024-01-28 16:27:24,047 INFO] Saving checkpoint models/model-base.enfr_step_6000.pt\n",
            "[2024-01-28 16:30:29,108 INFO] Step 6200/10000; acc: 75.1; ppl:  11.0; xent: 2.4; lr: 0.00112; sents:   74352; bsz: 3140/3846/93; 13323/16320 tok/s;   5328 sec;\n",
            "[2024-01-28 16:31:36,499 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1921)\n",
            "\n",
            "[2024-01-28 16:31:36,499 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2024-01-28 16:33:51,407 INFO] Step 6400/10000; acc: 75.9; ppl:  10.5; xent: 2.4; lr: 0.00110; sents:   74952; bsz: 3126/3842/94; 12363/15192 tok/s;   5531 sec;\n",
            "[2024-01-28 16:36:27,615 INFO] Step 6600/10000; acc: 76.0; ppl:  10.4; xent: 2.3; lr: 0.00109; sents:   74014; bsz: 3106/3819/93; 15906/19560 tok/s;   5687 sec;\n",
            "[2024-01-28 16:39:04,330 INFO] Step 6800/10000; acc: 75.9; ppl:  10.5; xent: 2.4; lr: 0.00107; sents:   75206; bsz: 3128/3827/94; 15968/19538 tok/s;   5844 sec;\n",
            "[2024-01-28 16:42:28,476 INFO] Step 7000/10000; acc: 76.1; ppl:  10.4; xent: 2.3; lr: 0.00106; sents:   74264; bsz: 3118/3840/93; 12221/15049 tok/s;   6048 sec;\n",
            "[2024-01-28 16:45:05,004 INFO] Step 7200/10000; acc: 76.8; ppl:  10.0; xent: 2.3; lr: 0.00104; sents:   76333; bsz: 3109/3811/95; 15890/19480 tok/s;   6204 sec;\n",
            "[2024-01-28 16:47:41,148 INFO] Step 7400/10000; acc: 76.7; ppl:  10.1; xent: 2.3; lr: 0.00103; sents:   73649; bsz: 3111/3845/92; 15939/19701 tok/s;   6360 sec;\n",
            "[2024-01-28 16:50:17,394 INFO] Step 7600/10000; acc: 76.5; ppl:  10.2; xent: 2.3; lr: 0.00101; sents:   74590; bsz: 3105/3840/93; 15898/19663 tok/s;   6517 sec;\n",
            "[2024-01-28 16:51:11,193 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3910)\n",
            "\n",
            "[2024-01-28 16:51:11,194 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2024-01-28 16:53:41,747 INFO] Step 7800/10000; acc: 77.5; ppl:   9.7; xent: 2.3; lr: 0.00100; sents:   73806; bsz: 3181/3851/92; 12454/15077 tok/s;   6721 sec;\n",
            "[2024-01-28 16:56:18,294 INFO] Step 8000/10000; acc: 77.5; ppl:   9.7; xent: 2.3; lr: 0.00099; sents:   77311; bsz: 3138/3847/97; 16038/19659 tok/s;   6878 sec;\n",
            "[2024-01-28 16:56:18,581 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=24)\n",
            "\n",
            "[2024-01-28 16:56:22,575 INFO] valid stats calculation\n",
            "                           took: 4.278542995452881 s.\n",
            "[2024-01-28 16:56:22,577 INFO] Train perplexity: 19.7607\n",
            "[2024-01-28 16:56:22,577 INFO] Train accuracy: 65.8623\n",
            "[2024-01-28 16:56:22,577 INFO] Sentences processed: 2.98656e+06\n",
            "[2024-01-28 16:56:22,577 INFO] Average bsz: 3122/3831/93\n",
            "[2024-01-28 16:56:22,577 INFO] Validation perplexity: 12.0825\n",
            "[2024-01-28 16:56:22,577 INFO] Validation accuracy: 74.4955\n",
            "[2024-01-28 16:56:22,578 INFO] Model is improving ppl: 12.4685 --> 12.0825.\n",
            "[2024-01-28 16:56:22,578 INFO] Model is improving acc: 73.6879 --> 74.4955.\n",
            "[2024-01-28 16:56:22,594 INFO] Saving checkpoint models/model-base.enfr_step_8000.pt\n",
            "[2024-01-28 16:59:26,955 INFO] Step 8200/10000; acc: 77.3; ppl:   9.8; xent: 2.3; lr: 0.00098; sents:   72400; bsz: 3137/3825/90; 13304/16219 tok/s;   7066 sec;\n",
            "[2024-01-28 17:01:51,984 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1919)\n",
            "\n",
            "[2024-01-28 17:01:51,985 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2024-01-28 17:02:48,930 INFO] Step 8400/10000; acc: 77.2; ppl:   9.8; xent: 2.3; lr: 0.00096; sents:   75346; bsz: 3067/3799/94; 12146/15046 tok/s;   7268 sec;\n",
            "[2024-01-28 17:05:25,267 INFO] Step 8600/10000; acc: 78.3; ppl:   9.3; xent: 2.2; lr: 0.00095; sents:   73355; bsz: 3113/3827/92; 15931/19581 tok/s;   7425 sec;\n",
            "[2024-01-28 17:08:01,611 INFO] Step 8800/10000; acc: 77.9; ppl:   9.5; xent: 2.3; lr: 0.00094; sents:   74511; bsz: 3118/3830/93; 15953/19598 tok/s;   7581 sec;\n",
            "[2024-01-28 17:10:38,210 INFO] Step 9000/10000; acc: 77.9; ppl:   9.5; xent: 2.3; lr: 0.00093; sents:   75363; bsz: 3142/3839/94; 16050/19614 tok/s;   7738 sec;\n",
            "[2024-01-28 17:14:06,030 INFO] Step 9200/10000; acc: 78.5; ppl:   9.2; xent: 2.2; lr: 0.00092; sents:   74124; bsz: 3099/3842/93; 11931/14789 tok/s;   7945 sec;\n",
            "[2024-01-28 17:16:42,655 INFO] Step 9400/10000; acc: 78.8; ppl:   9.1; xent: 2.2; lr: 0.00091; sents:   73234; bsz: 3146/3838/92; 16068/19604 tok/s;   8102 sec;\n",
            "[2024-01-28 17:19:19,349 INFO] Step 9600/10000; acc: 78.3; ppl:   9.3; xent: 2.2; lr: 0.00090; sents:   72091; bsz: 3103/3822/90; 15845/19515 tok/s;   8259 sec;\n",
            "[2024-01-28 17:21:31,621 INFO] * Transform statistics for corpus_1(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3879)\n",
            "\n",
            "[2024-01-28 17:21:31,622 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2024-01-28 17:22:46,388 INFO] Step 9800/10000; acc: 78.6; ppl:   9.2; xent: 2.2; lr: 0.00089; sents:   78319; bsz: 3143/3836/98; 12145/14824 tok/s;   8466 sec;\n",
            "[2024-01-28 17:25:23,307 INFO] Step 10000/10000; acc: 79.4; ppl:   8.9; xent: 2.2; lr: 0.00088; sents:   71780; bsz: 3131/3849/90; 15964/19622 tok/s;   8623 sec;\n",
            "[2024-01-28 17:25:23,592 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=24)\n",
            "\n",
            "[2024-01-28 17:25:27,222 INFO] valid stats calculation\n",
            "                           took: 3.912700891494751 s.\n",
            "[2024-01-28 17:25:27,224 INFO] Train perplexity: 17.0189\n",
            "[2024-01-28 17:25:27,224 INFO] Train accuracy: 68.3361\n",
            "[2024-01-28 17:25:27,224 INFO] Sentences processed: 3.72708e+06\n",
            "[2024-01-28 17:25:27,224 INFO] Average bsz: 3121/3831/93\n",
            "[2024-01-28 17:25:27,224 INFO] Validation perplexity: 11.9711\n",
            "[2024-01-28 17:25:27,225 INFO] Validation accuracy: 74.8932\n",
            "[2024-01-28 17:25:27,225 INFO] Model is improving ppl: 12.0825 --> 11.9711.\n",
            "[2024-01-28 17:25:27,225 INFO] Model is improving acc: 74.4955 --> 74.8932.\n",
            "[2024-01-28 17:25:27,242 INFO] Saving checkpoint models/model-base.enfr_step_10000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in case Colab is suddenly unable to navigate through directories\n",
        "import os\n",
        "path = '/content/drive/MyDrive/domain-adapted-nmt/nmt-tools'\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "qeAMnd2X2uuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -gpu 0 to use gpu\n",
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-general.en-filtered.en.subword.test -output general-fr.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "cdKHqwOJ1kKv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dc1f14-a762-41d5-ec3b-97393a96cb77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 14:46:04.447709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 14:46:04.447768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 14:46:04.449309: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 14:46:04.462116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 14:46:07.451589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 14:46:10.769925: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:46:10.770346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:46:10.770530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 14:46:12,327 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-29 14:46:34,947 INFO] Loading data into the model\n",
            "[2024-01-29 14:48:07,633 INFO] PRED SCORE: -0.3123, PRED PPL: 1.37 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  116.03815960884094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-technology.en-filtered.en.subword.test -output general-fr-tech.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d819b88a-49aa-4c29-e262-d427f70a50a2",
        "id": "PrJaqyyG_rVI"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 14:48:13.037642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 14:48:13.037714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 14:48:13.039130: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 14:48:13.046444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 14:48:14.190533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 14:48:15.882688: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:48:15.883294: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:48:15.883511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 14:48:17,245 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-29 14:48:20,434 INFO] Loading data into the model\n",
            "[2024-01-29 14:51:09,889 INFO] PRED SCORE: -0.7718, PRED PPL: 2.16 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  173.30067110061646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-base.enfr_step_10000.pt -src corpora/enfr/en-fr-medicine.en-filtered.en.subword.test -output general-fr-medicine.translated -gpu 0 -min_length 1\n"
      ],
      "metadata": {
        "id": "BDx0ml_c_vUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e4e3b6-03c8-4fe0-aed7-1f54328235e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 14:51:14.586189: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 14:51:14.586253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 14:51:14.588172: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 14:51:14.599555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 14:51:16.323520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 14:51:18.347375: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:51:18.348050: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:51:18.348305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 14:51:19,109 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-29 14:51:22,510 INFO] Loading data into the model\n",
            "[2024-01-29 14:54:34,408 INFO] PRED SCORE: -0.6860, PRED PPL: 1.99 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  195.3701252937317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-fr.translated"
      ],
      "metadata": {
        "id": "FVwRgj_O1nXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6ebbb61-c730-4293-e8a0-0755b5bfc72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁Il ▁ne ▁faut ▁pas ▁oublier ▁que ▁ 1 ▁ 2 6 0 ▁prisonniers ▁marocains ▁sont ▁toujours ▁détenus ▁dans ▁des ▁prisons ▁du ▁Front ▁POLISARIO , ▁où ▁ils ▁ont ▁été ▁maintenus ▁pendant ▁plus ▁de ▁ 2 5 ▁ans ▁en ▁violation ▁flagrante ▁du ▁droit ▁international ▁humanitaire .\n",
            "▁Fournir ▁au ▁Comité ▁des ▁exemplaires ▁du ▁texte ▁de ▁la ▁Convention ▁relative ▁aux ▁droits ▁de ▁l ' enfant ▁dans ▁toutes ▁les ▁langues ▁officielles ▁ou ▁dans ▁d ' autres ▁langues ▁ou ▁dialectes , ▁lorsque ▁cela ▁est ▁disponible .\n",
            "▁Nous ▁réaffirmons ▁que ▁les ▁lois ▁en ▁vigueur ▁dans ▁le ▁Sultanat ▁garantissent ▁la ▁protection ▁des ▁droits ▁de ▁l ' homme , ▁y ▁compris ▁les ▁droits ▁de ▁l ' enfant , ▁en ▁particulier ▁en ▁ce ▁qui ▁concerne ▁l ' implication ▁d ' enfants ▁dans ▁les ▁conflits ▁armés .\n",
            "▁Conformément ▁à ▁l ' accord ▁auquel ▁le ▁Conseil ▁est ▁parvenu ▁lors ▁de ▁ses ▁consultations ▁préalables , ▁j ' invite ▁les ▁membres ▁du ▁Conseil ▁à ▁poursuivre ▁l ' examen ▁de ▁la ▁question .\n",
            "▁Décisions ▁et ▁recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade -q sentencepiece"
      ],
      "metadata": {
        "id": "f76FH1TE1quR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa72fd40-f9af-4bcf-d3d2-52b8b8025413"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m0.9/1.3 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated file\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model general-fr.translated"
      ],
      "metadata": {
        "id": "K0e2k30QWshy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d52a7fd-7c3a-4286-aede-ad313dac1ba6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: general-fr.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model general-fr-tech.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model general-fr-medicine.translated"
      ],
      "metadata": {
        "id": "-sWyEHJd_1Om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6bc11e-61b6-409f-fb75-d9faca1a9aba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: general-fr-tech.translated.desubword\n",
            "Done desubwording! Output: general-fr-medicine.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 general-fr.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXEr-jHqWqXN",
        "outputId": "ac1c66be-7c53-427e-9405-fcfdafab281a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours détenus dans des prisons du Front POLISARIO, où ils ont été maintenus pendant plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "Fournir au Comité des exemplaires du texte de la Convention relative aux droits de l'enfant dans toutes les langues officielles ou dans d'autres langues ou dialectes, lorsque cela est disponible.\n",
            "Nous réaffirmons que les lois en vigueur dans le Sultanat garantissent la protection des droits de l'homme, y compris les droits de l'enfant, en particulier en ce qui concerne l'implication d'enfants dans les conflits armés.\n",
            "Conformément à l'accord auquel le Conseil est parvenu lors de ses consultations préalables, j'invite les membres du Conseil à poursuivre l'examen de la question.\n",
            "Décisions et recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model corpora/enfr/en-fr-general.fr-filtered.fr.subword.test"
      ],
      "metadata": {
        "id": "E_1RhBhL1tZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "637f705e-eda4-491d-d0fd-fec557dcf177"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3b19a43-ae11-43f0-c260-f16166143f27",
        "id": "Hy4p6uNsARgB"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword\n",
            "Done desubwording! Output: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword"
      ],
      "metadata": {
        "id": "ZG3qa1AX_o9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1c5268-f355-4fea-a113-2e66c078cfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "Faire parvenir au Comité des exemplaires du texte de la Convention relative aux droits de l'enfant dans toutes les langues officielles de l'État partie ainsi que dans ses autres langues ou dialectes, si elle a été traduite.\n",
            "Nous réaffirmons que la législation en vigueur dans le Sultanat garantit la protection des droits de l'homme, y compris les droits de l'enfant, en particulier pour ce qui est de l'implication d'enfants dans les conflits armés.\n",
            "Conformément à l'accord auquel le Conseil est parvenu lors de ses consultations préalables, j'invite à présent les membres du Conseil à poursuivre le débat sur la question dans le cadre de consultations.\n",
            "Décisions et recommandations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test bleu for baseline score\n",
        "# only need to run this cell once (once the script is in your Drive, you don't need to run this anymore)\n",
        "!wget https://raw.githubusercontent.com/ymoslem/MT-Evaluation/main/BLEU/compute-bleu.py\n",
        "!pip3 install sacrebleu"
      ],
      "metadata": {
        "id": "T5bw8Qqp1uzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 compute-bleu.py corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword general-fr.translated.desubword"
      ],
      "metadata": {
        "id": "ZidBkQGF5xAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c68eed9-9852-42a9-91c5-e82a1ebb63cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "MTed 1st sentence: Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours détenus dans des prisons du Front POLISARIO où ils ont été détenus depuis plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "BLEU:  43.191244985557496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 compute-bleu.py corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword general-fr-tech.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword general-fr-medicine.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76491a24-888e-4bc9-c13b-f16f8e839ad6",
        "id": "UeOC3dz9AYER"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Crée une nouvelle demande de réunion\n",
            "MTed 1st sentence: V.▁Adoption d'une nouvelle demande\n",
            "BLEU:  7.718966299615998\n",
            "Reference 1st sentence: Cependant, le temps nécessaire à l’ aggravation de la maladie après le traitement était le même dans les deux groupes (environ 10 mois).\n",
            "MTed 1st sentence: Cependant, le temps pris pour▁lutter contre la maladie après un traitement est le même dans les deux groupes (environ 10 mois).\n",
            "BLEU:  12.591000835462784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BLEU scores for \"out-of-domain\" text, or in this case, non-generic text, are quite horrible. This is expected since the baseline model hasn't seen any tech or med-specific inputs.\n",
        "\n",
        "Later on we can fine-tune this model with more generic corpora to improve it even more, but first we'll fine-tune our domain-adapted models for technology and medicine."
      ],
      "metadata": {
        "id": "oxuqKNlUEzpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for technology corpus"
      ],
      "metadata": {
        "id": "imMXZvK6DRIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tech = \"\"\"# tech.yaml\n",
        "# for technology corpus/model fine-tuning\n",
        "share_vocab: true\n",
        "src_vocab: run-tech/source.vocab\n",
        "tgt_vocab: run-tech/target.vocab\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "data:\n",
        "  # different corpus weighting for mixed fine-tuning approach\n",
        "  tech_corpus:\n",
        "    path_src: corpora/enfr/en-fr-technology.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 10\n",
        "  # randomly sample portions from the generic/general data we used to train the baseline model for mixed fine-tuning\n",
        "  gen_corpus:\n",
        "    path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 1\n",
        "  valid: # validation set\n",
        "    path_src: corpora/enfr/en-fr-technology.en-filtered.en.subword.dev\n",
        "    path_tgt: corpora/enfr/en-fr-technology.fr-filtered.fr.subword.dev\n",
        "    transforms: [filtertoolong]\n",
        "\n",
        "update_vocab: true\n",
        "train_from: 'models/model-base.enfr_step_10000.pt' # the base model trained earlier\n",
        "reset_optim: all\n",
        "\n",
        "# filtertoolong\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# tokenization\n",
        "src_subword_model: source-general.model\n",
        "tgt_subword_model: target-general.model\n",
        "\n",
        "save_data: run-tech\n",
        "save_model: models/model-tech.enfr\n",
        "log_file: train-tech.log\n",
        "early_stopping: 4\n",
        "\n",
        "keep_checkpoint: 4\n",
        "save_checkpoint_steps: 1000\n",
        "average_decay: 0.0005\n",
        "seed: 1234\n",
        "warmup_steps: 1000\n",
        "report_every: 100\n",
        "\n",
        "train_steps: 3000\n",
        "valid_steps: 1000\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"tech.yaml\", \"w+\") as tech_yaml:\n",
        "  tech_yaml.write(tech)"
      ],
      "metadata": {
        "id": "05xpXlOBwjAJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat source-technology.vocab | python3 spm_to_vocab.py > source-technology.onmt_vocab"
      ],
      "metadata": {
        "id": "b6Gfy2MMiaZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_build_vocab -config tech.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fze04KECkIJ",
        "outputId": "8e6452f2-58c8-4d14-fd21-2a36c9f6f873"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 14:58:18.799477: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 14:58:18.799560: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 14:58:18.801580: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 14:58:18.812923: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 14:58:20.552708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 14:58:23.142471: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:58:23.143048: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:58:23.143263: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 14:58:23,909 INFO] Counter vocab from -1 samples.\n",
            "[2024-01-29 14:58:23,909 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2024-01-29 14:58:26,899 INFO] * Transform statistics for tech_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=38)\n",
            "\n",
            "[2024-01-29 14:58:26,901 INFO] * Transform statistics for tech_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=35)\n",
            "\n",
            "[2024-01-29 14:59:01,086 INFO] * Transform statistics for gen_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1550)\n",
            "\n",
            "[2024-01-29 14:59:01,141 INFO] * Transform statistics for gen_corpus(50.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1589)\n",
            "\n",
            "[2024-01-29 14:59:01,336 INFO] Counters src: 55902\n",
            "[2024-01-29 14:59:01,336 INFO] Counters tgt: 57951\n",
            "[2024-01-29 14:59:01,377 INFO] Counters after share:95950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_train -config tech.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq4QmVfZCmEH",
        "outputId": "03a936e3-be37-4763-dc39-316d7aba789a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 14:59:06.074962: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 14:59:06.075019: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 14:59:06.076355: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 14:59:06.083819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 14:59:07.179789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 14:59:08.725939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:59:08.726412: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 14:59:08.726584: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 14:59:09,411 INFO] Parsed 3 corpora from -data.\n",
            "[2024-01-29 14:59:09,413 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-29 14:59:11,737 INFO] Updating checkpoint vocabulary with new vocabulary\n",
            "[2024-01-29 14:59:11,738 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-29 14:59:11,958 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', ',', '▁the', '.', '▁de', \"'\", '▁']\n",
            "[2024-01-29 14:59:11,959 INFO] The decoder start token is: <s>\n",
            "[2024-01-29 14:59:11,984 INFO] Building model...\n",
            "[2024-01-29 14:59:13,551 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-29 14:59:13,551 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-29 14:59:13,552 INFO] Updating vocabulary embeddings with checkpoint embeddings\n",
            "[2024-01-29 14:59:13,879 INFO] src: 21345 new tokens\n",
            "[2024-01-29 14:59:14,675 INFO] tgt: 17895 new tokens\n",
            "[2024-01-29 14:59:15,179 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=50000, bias=True)\n",
            ")\n",
            "[2024-01-29 14:59:15,186 INFO] encoder: 44487680\n",
            "[2024-01-29 14:59:15,186 INFO] decoder: 76435280\n",
            "[2024-01-29 14:59:15,186 INFO] * number of parameters: 120922960\n",
            "[2024-01-29 14:59:15,187 INFO] Trainable parameters = {'torch.float32': 120922960, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-29 14:59:15,188 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-29 14:59:15,188 INFO]  * src vocab size = 50000\n",
            "[2024-01-29 14:59:15,188 INFO]  * tgt vocab size = 50000\n",
            "[2024-01-29 14:59:15,844 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 1\n",
            "[2024-01-29 14:59:15,844 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 1\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 14:59:15,845 INFO] Starting training on GPU: [0]\n",
            "[2024-01-29 14:59:15,845 INFO] Start training loop and validate every 1000 steps...\n",
            "[2024-01-29 14:59:15,845 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=150))\n",
            "[2024-01-29 14:59:18,122 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 2\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 14:59:19,670 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 3\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 14:59:21,388 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 4\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 14:59:23,182 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 5\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 14:59:25,101 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 6\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:00:57,728 INFO] Step 100/ 3000; acc: 58.6; ppl:  51.1; xent: 3.9; lr: 0.00028; sents:   97773; bsz: 2618/3429/244; 10279/13461 tok/s;    102 sec;\n",
            "[2024-01-29 15:01:42,475 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=388)\n",
            "\n",
            "[2024-01-29 15:01:42,475 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 7\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:01:46,322 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 8\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:01:47,452 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 9\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:01:52,851 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 10\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:01:54,550 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 11\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:02:48,372 INFO] Step 200/ 3000; acc: 67.1; ppl:  20.7; xent: 3.0; lr: 0.00056; sents:   97988; bsz: 2600/3419/245; 9398/12360 tok/s;    213 sec;\n",
            "[2024-01-29 15:04:10,305 INFO] Step 300/ 3000; acc: 73.0; ppl:  13.9; xent: 2.6; lr: 0.00084; sents:  100758; bsz: 2618/3426/252; 12780/16724 tok/s;    294 sec;\n",
            "[2024-01-29 15:04:16,587 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-29 15:04:16,587 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 12\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:04:17,767 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 13\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:04:19,051 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 14\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:04:24,186 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 15\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:04:26,033 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 16\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:06:00,231 INFO] Step 400/ 3000; acc: 77.1; ppl:  10.8; xent: 2.4; lr: 0.00112; sents:   95049; bsz: 2588/3425/238; 9416/12462 tok/s;    404 sec;\n",
            "[2024-01-29 15:06:50,287 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-29 15:06:50,288 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 17\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:06:51,478 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 18\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:06:52,650 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 19\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:06:57,724 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 20\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:06:59,573 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 21\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:07:01,045 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 22\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:07:52,453 INFO] Step 500/ 3000; acc: 78.7; ppl:   9.9; xent: 2.3; lr: 0.00140; sents:   96099; bsz: 2630/3404/240; 9374/12134 tok/s;    517 sec;\n",
            "[2024-01-29 15:09:15,216 INFO] Step 600/ 3000; acc: 80.0; ppl:   9.3; xent: 2.2; lr: 0.00168; sents:  104365; bsz: 2619/3457/261; 12656/16706 tok/s;    599 sec;\n",
            "[2024-01-29 15:09:27,768 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=383)\n",
            "\n",
            "[2024-01-29 15:09:27,768 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 23\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:09:28,937 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 24\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:09:34,031 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 25\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:09:35,717 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 26\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:09:36,838 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 27\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:11:06,350 INFO] Step 700/ 3000; acc: 80.4; ppl:   9.0; xent: 2.2; lr: 0.00196; sents:   93116; bsz: 2576/3415/233; 9272/12292 tok/s;    711 sec;\n",
            "[2024-01-29 15:12:03,497 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-29 15:12:03,497 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 28\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:12:04,647 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 29\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:12:10,544 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 30\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:12:11,704 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 31\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:12:12,828 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 32\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:12:58,379 INFO] Step 800/ 3000; acc: 80.5; ppl:   9.1; xent: 2.2; lr: 0.00224; sents:  102413; bsz: 2665/3449/256; 9515/12315 tok/s;    823 sec;\n",
            "[2024-01-29 15:14:20,918 INFO] Step 900/ 3000; acc: 80.7; ppl:   9.0; xent: 2.2; lr: 0.00252; sents:  100314; bsz: 2624/3441/251; 12715/16675 tok/s;    905 sec;\n",
            "[2024-01-29 15:14:39,135 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-29 15:14:39,135 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 33\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:14:44,047 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 34\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:14:45,743 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 35\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:14:46,861 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 36\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:14:52,030 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 37\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:16:15,759 INFO] Step 1000/ 3000; acc: 79.9; ppl:   9.4; xent: 2.2; lr: 0.00279; sents:   98528; bsz: 2551/3381/246; 8886/11775 tok/s;   1020 sec;\n",
            "[2024-01-29 15:16:18,625 INFO] valid stats calculation\n",
            "                           took: 2.862334966659546 s.\n",
            "[2024-01-29 15:16:18,628 INFO] Train perplexity: 12.6253\n",
            "[2024-01-29 15:16:18,628 INFO] Train accuracy: 75.5938\n",
            "[2024-01-29 15:16:18,628 INFO] Sentences processed: 986403\n",
            "[2024-01-29 15:16:18,628 INFO] Average bsz: 2609/3424/247\n",
            "[2024-01-29 15:16:18,628 INFO] Validation perplexity: 11.3846\n",
            "[2024-01-29 15:16:18,628 INFO] Validation accuracy: 78.8503\n",
            "[2024-01-29 15:16:18,628 INFO] Model is improving ppl: inf --> 11.3846.\n",
            "[2024-01-29 15:16:18,628 INFO] Model is improving acc: -inf --> 78.8503.\n",
            "[2024-01-29 15:16:18,646 INFO] Saving checkpoint models/model-tech.enfr_step_1000.pt\n",
            "[2024-01-29 15:17:31,159 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-29 15:17:31,159 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 38\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:17:33,178 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 39\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:17:34,317 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 40\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:17:39,336 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 41\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:17:40,451 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 42\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:17:41,710 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 43\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:18:19,693 INFO] Step 1100/ 3000; acc: 80.1; ppl:   9.3; xent: 2.2; lr: 0.00266; sents:   95778; bsz: 2644/3422/239; 8533/11045 tok/s;   1144 sec;\n",
            "[2024-01-29 15:19:41,512 INFO] Step 1200/ 3000; acc: 81.0; ppl:   9.0; xent: 2.2; lr: 0.00255; sents:   99014; bsz: 2588/3427/248; 12653/16756 tok/s;   1226 sec;\n",
            "[2024-01-29 15:20:07,085 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=388)\n",
            "\n",
            "[2024-01-29 15:20:07,086 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 44\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:20:08,264 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 45\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:20:13,173 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 46\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:20:14,586 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 47\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:20:16,749 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 48\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:21:33,956 INFO] Step 1300/ 3000; acc: 80.9; ppl:   9.0; xent: 2.2; lr: 0.00245; sents:   94085; bsz: 2597/3405/235; 9240/12113 tok/s;   1338 sec;\n",
            "[2024-01-29 15:22:43,587 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=386)\n",
            "\n",
            "[2024-01-29 15:22:43,588 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 49\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:22:44,759 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 50\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:22:50,268 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 51\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:22:52,414 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 52\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:22:58,504 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 53\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:23:26,198 INFO] Step 1400/ 3000; acc: 82.4; ppl:   8.5; xent: 2.1; lr: 0.00236; sents:  102335; bsz: 2610/3426/256; 9303/12210 tok/s;   1450 sec;\n",
            "[2024-01-29 15:24:48,259 INFO] Step 1500/ 3000; acc: 82.4; ppl:   8.5; xent: 2.1; lr: 0.00228; sents:   96760; bsz: 2608/3434/242; 12714/16736 tok/s;   1532 sec;\n",
            "[2024-01-29 15:25:19,833 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-29 15:25:19,834 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 54\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:25:25,400 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 55\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:25:26,944 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 56\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:25:32,071 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 57\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:25:33,204 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 58\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:25:34,336 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 59\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:26:43,524 INFO] Step 1600/ 3000; acc: 83.4; ppl:   8.2; xent: 2.1; lr: 0.00221; sents:   99461; bsz: 2614/3420/249; 9072/11869 tok/s;   1648 sec;\n",
            "[2024-01-29 15:28:00,611 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=383)\n",
            "\n",
            "[2024-01-29 15:28:00,612 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 60\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:28:01,788 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 61\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:28:06,720 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 62\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:28:07,839 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 63\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:28:08,975 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 64\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:28:35,750 INFO] Step 1700/ 3000; acc: 83.1; ppl:   8.3; xent: 2.1; lr: 0.00214; sents:  100004; bsz: 2579/3393/250; 9191/12093 tok/s;   1760 sec;\n",
            "[2024-01-29 15:29:57,523 INFO] Step 1800/ 3000; acc: 83.5; ppl:   8.2; xent: 2.1; lr: 0.00208; sents:   97812; bsz: 2596/3427/245; 12699/16765 tok/s;   1842 sec;\n",
            "[2024-01-29 15:30:36,680 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=385)\n",
            "\n",
            "[2024-01-29 15:30:36,680 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 65\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:30:37,883 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 66\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:30:42,748 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 67\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:30:44,009 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 68\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:30:46,002 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 69\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:31:49,669 INFO] Step 1900/ 3000; acc: 83.0; ppl:   8.3; xent: 2.1; lr: 0.00203; sents:   89776; bsz: 2604/3372/224; 9286/12029 tok/s;   1954 sec;\n",
            "[2024-01-29 15:33:11,580 INFO] Step 2000/ 3000; acc: 84.5; ppl:   7.8; xent: 2.1; lr: 0.00198; sents:  103136; bsz: 2605/3431/258; 12721/16753 tok/s;   2036 sec;\n",
            "[2024-01-29 15:33:11,658 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=5)\n",
            "\n",
            "[2024-01-29 15:33:13,143 INFO] valid stats calculation\n",
            "                           took: 1.5609848499298096 s.\n",
            "[2024-01-29 15:33:13,146 INFO] Train perplexity: 10.3651\n",
            "[2024-01-29 15:33:13,146 INFO] Train accuracy: 79.0019\n",
            "[2024-01-29 15:33:13,146 INFO] Sentences processed: 1.96456e+06\n",
            "[2024-01-29 15:33:13,146 INFO] Average bsz: 2607/3420/246\n",
            "[2024-01-29 15:33:13,146 INFO] Validation perplexity: 10.5999\n",
            "[2024-01-29 15:33:13,146 INFO] Validation accuracy: 81.0742\n",
            "[2024-01-29 15:33:13,146 INFO] Model is improving ppl: 11.3846 --> 10.5999.\n",
            "[2024-01-29 15:33:13,146 INFO] Model is improving acc: 78.8503 --> 81.0742.\n",
            "[2024-01-29 15:33:13,165 INFO] Saving checkpoint models/model-tech.enfr_step_2000.pt\n",
            "[2024-01-29 15:33:24,032 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=389)\n",
            "\n",
            "[2024-01-29 15:33:24,032 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 70\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:33:29,039 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 71\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:33:31,388 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 72\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:33:33,026 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 73\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:33:39,555 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 74\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:35:23,629 INFO] Step 2100/ 3000; acc: 84.5; ppl:   7.9; xent: 2.1; lr: 0.00193; sents:  100144; bsz: 2601/3432/250; 7878/10396 tok/s;   2168 sec;\n",
            "[2024-01-29 15:36:08,765 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=384)\n",
            "\n",
            "[2024-01-29 15:36:08,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 75\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:36:09,988 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 76\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:36:11,161 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 77\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:36:16,245 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 78\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:36:17,784 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 79\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:36:19,714 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 80\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:37:14,461 INFO] Step 2200/ 3000; acc: 84.7; ppl:   7.8; xent: 2.1; lr: 0.00188; sents:  102139; bsz: 2617/3429/255; 9445/12374 tok/s;   2279 sec;\n",
            "[2024-01-29 15:38:34,935 INFO] Step 2300/ 3000; acc: 84.1; ppl:   7.9; xent: 2.1; lr: 0.00184; sents:   93546; bsz: 2617/3417/234; 13007/16985 tok/s;   2359 sec;\n",
            "[2024-01-29 15:38:43,637 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=386)\n",
            "\n",
            "[2024-01-29 15:38:43,637 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 81\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:38:44,897 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 82\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:38:49,988 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 83\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:38:51,761 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 84\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:38:52,884 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 85\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:40:25,763 INFO] Step 2400/ 3000; acc: 84.4; ppl:   7.9; xent: 2.1; lr: 0.00180; sents:   98408; bsz: 2589/3406/246; 9345/12295 tok/s;   2470 sec;\n",
            "[2024-01-29 15:41:17,210 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-29 15:41:17,211 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 86\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:41:18,352 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 87\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:41:23,991 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 88\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:41:25,977 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 89\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:41:27,848 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 90\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:42:16,257 INFO] Step 2500/ 3000; acc: 85.4; ppl:   7.6; xent: 2.0; lr: 0.00177; sents:   97730; bsz: 2625/3445/244; 9501/12470 tok/s;   2580 sec;\n",
            "[2024-01-29 15:43:37,535 INFO] Step 2600/ 3000; acc: 84.7; ppl:   7.7; xent: 2.0; lr: 0.00173; sents:   99087; bsz: 2616/3438/248; 12874/16919 tok/s;   2662 sec;\n",
            "[2024-01-29 15:43:50,759 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-29 15:43:50,760 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 91\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:43:55,233 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 92\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:43:56,659 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 93\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:43:58,744 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 94\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:44:05,223 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 95\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-29 15:44:05,435 INFO] * Transform statistics for gen_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=3001)\n",
            "\n",
            "[2024-01-29 15:44:05,436 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 95\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:44:07,157 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 96\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:45:32,931 INFO] Step 2700/ 3000; acc: 84.9; ppl:   7.7; xent: 2.0; lr: 0.00170; sents:   97964; bsz: 2605/3435/245; 9030/11906 tok/s;   2777 sec;\n",
            "[2024-01-29 15:46:30,982 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=381)\n",
            "\n",
            "[2024-01-29 15:46:30,983 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 97\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:46:32,198 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 98\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:46:37,379 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 99\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:46:39,225 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 100\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:46:40,445 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 101\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:47:24,472 INFO] Step 2800/ 3000; acc: 85.4; ppl:   7.6; xent: 2.0; lr: 0.00167; sents:   97037; bsz: 2605/3415/243; 9342/12246 tok/s;   2889 sec;\n",
            "[2024-01-29 15:48:47,218 INFO] Step 2900/ 3000; acc: 85.7; ppl:   7.4; xent: 2.0; lr: 0.00164; sents:   99768; bsz: 2632/3438/249; 12721/16622 tok/s;   2971 sec;\n",
            "[2024-01-29 15:49:07,342 INFO] * Transform statistics for tech_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=387)\n",
            "\n",
            "[2024-01-29 15:49:07,343 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 102\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:49:08,551 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 103\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:49:13,501 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 104\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:49:14,985 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 105\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:49:16,984 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* tech_corpus: 106\n",
            "\t\t\t* gen_corpus: 2\n",
            "[2024-01-29 15:50:41,570 INFO] Step 3000/ 3000; acc: 85.8; ppl:   7.4; xent: 2.0; lr: 0.00161; sents:  103148; bsz: 2602/3406/258; 9101/11914 tok/s;   3086 sec;\n",
            "[2024-01-29 15:50:41,641 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=5)\n",
            "\n",
            "[2024-01-29 15:50:43,070 INFO] valid stats calculation\n",
            "                           took: 1.4977147579193115 s.\n",
            "[2024-01-29 15:50:43,072 INFO] Train perplexity: 9.37889\n",
            "[2024-01-29 15:50:43,072 INFO] Train accuracy: 80.9856\n",
            "[2024-01-29 15:50:43,072 INFO] Sentences processed: 2.95354e+06\n",
            "[2024-01-29 15:50:43,072 INFO] Average bsz: 2608/3422/246\n",
            "[2024-01-29 15:50:43,072 INFO] Validation perplexity: 10.7006\n",
            "[2024-01-29 15:50:43,072 INFO] Validation accuracy: 81.7934\n",
            "[2024-01-29 15:50:43,072 INFO] Stalled patience: 3/4\n",
            "[2024-01-29 15:50:43,090 INFO] Saving checkpoint models/model-tech.enfr_step_3000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# translate domain-specific and generic corpora\n",
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-technology.en-filtered.en.subword.test -output technology-fr.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxXARacPCtZq",
        "outputId": "47144181-30f0-4277-f60b-80b40cfb7075"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 15:51:03.240545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 15:51:03.240615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 15:51:03.242652: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 15:51:03.253953: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 15:51:05.291216: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 15:51:07.487728: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:51:07.488243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:51:07.488422: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 15:51:08,376 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-29 15:51:11,398 INFO] Loading data into the model\n",
            "[2024-01-29 15:51:49,481 INFO] PRED SCORE: -0.1734, PRED PPL: 1.19 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  41.139256715774536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-medicine.en-filtered.en.subword.test -output technology-fr-medicine.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsuWq1_SC5w0",
        "outputId": "3e642865-b67e-4f70-e4ba-9d8e16a4a337"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 15:51:53.570142: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 15:51:53.570205: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 15:51:53.572278: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 15:51:53.583765: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 15:51:55.202481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 15:51:57.174776: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:51:57.175267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:51:57.175446: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 15:51:57,621 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-29 15:52:00,422 INFO] Loading data into the model\n",
            "[2024-01-29 15:53:03,409 INFO] PRED SCORE: -0.4665, PRED PPL: 1.59 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  65.82321262359619\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-tech.enfr_step_3000.pt -src corpora/enfr/en-fr-general.en-filtered.en.subword.test -output technology-fr-general.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4SNaRK-C-gF",
        "outputId": "58315413-7d6b-4c1c-c5ae-0b22ac87c209"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-29 15:53:07.381295: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-29 15:53:07.381355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-29 15:53:07.382711: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-29 15:53:07.390340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-29 15:53:08.544461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-29 15:53:10.109405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:53:10.109870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-29 15:53:10.110069: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-29 15:53:10,569 INFO] Loading checkpoint from models/model-tech.enfr_step_3000.pt\n",
            "[2024-01-29 15:53:14,341 INFO] Loading data into the model\n",
            "[2024-01-29 15:54:44,797 INFO] PRED SCORE: -0.3177, PRED PPL: 1.37 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  94.26260137557983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated files\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model technology-fr.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model technology-fr-medicine.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model technology-fr-general.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N5lknruDkMu",
        "outputId": "60b08aec-fc45-4aee-9192-35ecd63c3175"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: technology-fr.translated.desubword\n",
            "Done desubwording! Output: technology-fr-medicine.translated.desubword\n",
            "Done desubwording! Output: technology-fr-general.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU scores for technology-specific NMT model\n",
        "# the filtered test datasets were already desubworded and saved previously when evaluating the baseline model, so we can just reuse them\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword technology-fr.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword technology-fr-medicine.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword technology-fr-general.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udquum9SDymf",
        "outputId": "0042749d-0ac2-4c42-9659-0f300ab541c4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Crée une nouvelle demande de réunion\n",
            "MTed 1st sentence: Crée une nouvelle demande de réunionNew\n",
            "BLEU:  57.09065291734113\n",
            "Reference 1st sentence: Cependant, le temps nécessaire à l’ aggravation de la maladie après le traitement était le même dans les deux groupes (environ 10 mois).\n",
            "MTed 1st sentence: Toutefois, le temps nécessaire pour que la maladie soit pire après le traitement est le même dans les deux groupes (environ 10 mois).\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  16.21355600694869\n",
            "Reference 1st sentence: Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "MTed 1st sentence: Il ne faut pas oublier que 1 260 prisonniers marocains sont toujours détenus dans des prisons du Front POLISARIO, où ils ont été gardés pendant plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "BLEU:  42.48529922655942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for medicine corpus"
      ],
      "metadata": {
        "id": "-HlglKIBEQtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "med = \"\"\"# med.yaml\n",
        "# for medicine corpus/model fine-tuning\n",
        "share_vocab: true\n",
        "src_vocab: run-med/source.vocab\n",
        "tgt_vocab: run-med/target.vocab\n",
        "src_vocab_size: 50000\n",
        "tgt_vocab_size: 50000\n",
        "\n",
        "data:\n",
        "  # different corpus weighting for mixed fine-tuning approach\n",
        "  med_corpus:\n",
        "    path_src: corpora/enfr/en-fr-medicine.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 10\n",
        "  # randomly sample portions from the generic/general data we used to train the baseline model for mixed fine-tuning\n",
        "  gen_corpus:\n",
        "    path_src: corpora/enfr/en-fr-general.en-filtered.en.subword.train\n",
        "    path_tgt: corpora/enfr/en-fr-general.fr-filtered.fr.subword.train\n",
        "    transforms: [filtertoolong]\n",
        "    weight: 1\n",
        "  valid:\n",
        "    path_src: corpora/enfr/en-fr-medicine.en-filtered.en.subword.dev\n",
        "    path_tgt: corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.dev\n",
        "    transforms: [filtertoolong]\n",
        "\n",
        "update_vocab: true\n",
        "train_from: 'models/model-base.enfr_step_10000.pt' # the base model trained earlier\n",
        "reset_optim: all\n",
        "\n",
        "# filtertoolong\n",
        "src_seq_length: 150\n",
        "tgt_seq_length: 150\n",
        "\n",
        "# tokenization\n",
        "src_subword_model: source-general.model\n",
        "tgt_subword_model: target-general.model\n",
        "\n",
        "save_data: run-tech\n",
        "save_model: models/model-med.enfr\n",
        "log_file: train-tech.log\n",
        "early_stopping: 4\n",
        "\n",
        "keep_checkpoint: 4\n",
        "save_checkpoint_steps: 1000\n",
        "average_decay: 0.0005\n",
        "seed: 1234\n",
        "warmup_steps: 2000\n",
        "report_every: 200\n",
        "\n",
        "train_steps: 6000\n",
        "valid_steps: 2000\n",
        "\n",
        "# Number of GPUs, and IDs of GPUs\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "\n",
        "# Batching\n",
        "bucket_size: 262144\n",
        "num_workers: 0\n",
        "batch_type: \"tokens\"\n",
        "batch_size: 4096\n",
        "valid_batch_size: 2048\n",
        "max_generator_batches: 2\n",
        "accum_count: [4]\n",
        "accum_steps: [0]\n",
        "\n",
        "# Optimization\n",
        "model_dtype: \"fp16\"\n",
        "optim: \"adam\"\n",
        "learning_rate: 2\n",
        "# warmup_steps: 8000\n",
        "decay_method: \"noam\"\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "param_init: 0\n",
        "param_init_glorot: true\n",
        "normalization: \"tokens\"\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "position_encoding: true\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout_steps: [0]\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"med.yaml\", \"w+\") as med_yaml:\n",
        "  med_yaml.write(med)"
      ],
      "metadata": {
        "id": "t90B79nrErWG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat source-medicine.vocab | python3 spm_to_vocab.py > source-medicine.onmt_vocab"
      ],
      "metadata": {
        "id": "ZcxIJcfwjAMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c66318-e364-4869-cc24-458f39c46ede"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-30 14:52:27.024012: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 14:52:27.024095: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 14:52:27.025995: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 14:52:27.036812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 14:52:28.814551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-30 14:52:30.202486: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 14:52:30.202918: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 14:52:30.203134: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_build_vocab -config med.yaml -n_sample -1 -num_threads 2"
      ],
      "metadata": {
        "id": "cuKVWXT4Er5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_train -config med.yaml"
      ],
      "metadata": {
        "id": "F78AhUXbFXut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee4f365-66f0-4074-bc55-99f2954f8a44"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-30 15:16:54.886987: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 15:16:54.887060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 15:16:54.888363: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 15:16:54.895406: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 15:16:56.063715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-30 15:16:57.158636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 15:16:57.159095: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 15:16:57.159280: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-30 15:16:57,888 INFO] Parsed 3 corpora from -data.\n",
            "[2024-01-30 15:16:57,890 INFO] Loading checkpoint from models/model-base.enfr_step_10000.pt\n",
            "[2024-01-30 15:17:03,850 INFO] Updating checkpoint vocabulary with new vocabulary\n",
            "[2024-01-30 15:17:03,850 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2024-01-30 15:17:04,115 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', ',', '.', '▁', '▁de', '▁the', \"'\"]\n",
            "[2024-01-30 15:17:04,116 INFO] The decoder start token is: <s>\n",
            "[2024-01-30 15:17:04,135 INFO] Building model...\n",
            "[2024-01-30 15:17:05,666 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2024-01-30 15:17:05,667 INFO] Non quantized layer compute is fp16\n",
            "[2024-01-30 15:17:05,668 INFO] Updating vocabulary embeddings with checkpoint embeddings\n",
            "[2024-01-30 15:17:05,946 INFO] src: 26390 new tokens\n",
            "[2024-01-30 15:17:06,520 INFO] tgt: 23170 new tokens\n",
            "[2024-01-30 15:17:06,856 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(50000, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=512, out_features=50000, bias=True)\n",
            ")\n",
            "[2024-01-30 15:17:06,859 INFO] encoder: 44487680\n",
            "[2024-01-30 15:17:06,860 INFO] decoder: 76435280\n",
            "[2024-01-30 15:17:06,860 INFO] * number of parameters: 120922960\n",
            "[2024-01-30 15:17:06,860 INFO] Trainable parameters = {'torch.float32': 120922960, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-30 15:17:06,861 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2024-01-30 15:17:06,861 INFO]  * src vocab size = 50000\n",
            "[2024-01-30 15:17:06,861 INFO]  * tgt vocab size = 50000\n",
            "[2024-01-30 15:17:07,535 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 1\n",
            "[2024-01-30 15:17:07,536 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 1\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:17:07,536 INFO] Starting training on GPU: [0]\n",
            "[2024-01-30 15:17:07,536 INFO] Start training loop and validate every 2000 steps...\n",
            "[2024-01-30 15:17:07,536 INFO] Scoring with: TransformPipe(FilterTooLongTransform(src_seq_length=150, tgt_seq_length=150))\n",
            "[2024-01-30 15:20:10,518 INFO] Step 200/ 6000; acc: 51.5; ppl:  83.2; xent: 4.4; lr: 0.00020; sents:  109404; bsz: 3069/3699/137; 13417/16171 tok/s;    183 sec;\n",
            "[2024-01-30 15:22:47,573 INFO] Step 400/ 6000; acc: 62.7; ppl:  27.6; xent: 3.3; lr: 0.00040; sents:  106777; bsz: 3069/3698/133; 15635/18835 tok/s;    340 sec;\n",
            "[2024-01-30 15:23:33,909 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=550)\n",
            "\n",
            "[2024-01-30 15:23:33,910 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 2\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:26:03,050 INFO] Step 600/ 6000; acc: 68.9; ppl:  17.5; xent: 2.9; lr: 0.00059; sents:  111168; bsz: 3058/3704/139; 12516/15161 tok/s;    536 sec;\n",
            "[2024-01-30 15:28:41,193 INFO] Step 800/ 6000; acc: 72.7; ppl:  13.5; xent: 2.6; lr: 0.00079; sents:  106629; bsz: 3068/3716/133; 15519/18800 tok/s;    694 sec;\n",
            "[2024-01-30 15:30:09,765 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=570)\n",
            "\n",
            "[2024-01-30 15:30:09,765 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 3\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:31:54,879 INFO] Step 1000/ 6000; acc: 75.3; ppl:  11.6; xent: 2.5; lr: 0.00099; sents:  116705; bsz: 3093/3707/146; 12777/15311 tok/s;    887 sec;\n",
            "[2024-01-30 15:31:54,899 INFO] Saving checkpoint models/model-med.enfr_step_1000.pt\n",
            "[2024-01-30 15:34:45,596 INFO] Step 1200/ 6000; acc: 75.9; ppl:  11.0; xent: 2.4; lr: 0.00119; sents:  108995; bsz: 3061/3701/136; 14343/17345 tok/s;   1058 sec;\n",
            "[2024-01-30 15:36:56,166 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=587)\n",
            "\n",
            "[2024-01-30 15:36:56,167 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 4\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:37:59,674 INFO] Step 1400/ 6000; acc: 76.5; ppl:  10.6; xent: 2.4; lr: 0.00138; sents:  104894; bsz: 3062/3717/131; 12620/15321 tok/s;   1252 sec;\n",
            "[2024-01-30 15:40:37,420 INFO] Step 1600/ 6000; acc: 77.4; ppl:  10.1; xent: 2.3; lr: 0.00158; sents:  107295; bsz: 3050/3710/134; 15465/18815 tok/s;   1410 sec;\n",
            "[2024-01-30 15:43:15,852 INFO] Step 1800/ 6000; acc: 76.6; ppl:  10.4; xent: 2.3; lr: 0.00178; sents:  111889; bsz: 3084/3680/140; 15574/18580 tok/s;   1568 sec;\n",
            "[2024-01-30 15:46:30,824 INFO] Step 2000/ 6000; acc: 77.8; ppl:   9.9; xent: 2.3; lr: 0.00198; sents:  108406; bsz: 3083/3692/136; 12650/15149 tok/s;   1763 sec;\n",
            "[2024-01-30 15:46:34,605 INFO] valid stats calculation\n",
            "                           took: 3.7778942584991455 s.\n",
            "[2024-01-30 15:46:34,607 INFO] Train perplexity: 15.4224\n",
            "[2024-01-30 15:46:34,607 INFO] Train accuracy: 71.5362\n",
            "[2024-01-30 15:46:34,607 INFO] Sentences processed: 1.09216e+06\n",
            "[2024-01-30 15:46:34,607 INFO] Average bsz: 3070/3702/137\n",
            "[2024-01-30 15:46:34,607 INFO] Validation perplexity: 9.01015\n",
            "[2024-01-30 15:46:34,607 INFO] Validation accuracy: 80.6287\n",
            "[2024-01-30 15:46:34,607 INFO] Model is improving ppl: inf --> 9.01015.\n",
            "[2024-01-30 15:46:34,607 INFO] Model is improving acc: -inf --> 80.6287.\n",
            "[2024-01-30 15:46:34,624 INFO] Saving checkpoint models/model-med.enfr_step_2000.pt\n",
            "[2024-01-30 15:49:28,946 INFO] Step 2200/ 6000; acc: 77.5; ppl:  10.0; xent: 2.3; lr: 0.00188; sents:  110010; bsz: 3064/3709/138; 13762/16658 tok/s;   1941 sec;\n",
            "[2024-01-30 15:50:10,745 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1113)\n",
            "\n",
            "[2024-01-30 15:50:10,746 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 5\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:52:48,822 INFO] Step 2400/ 6000; acc: 78.7; ppl:   9.4; xent: 2.2; lr: 0.00180; sents:  106917; bsz: 3038/3712/134; 12158/14859 tok/s;   2141 sec;\n",
            "[2024-01-30 15:55:27,069 INFO] Step 2600/ 6000; acc: 78.6; ppl:   9.4; xent: 2.2; lr: 0.00173; sents:  110816; bsz: 3065/3710/139; 15493/18754 tok/s;   2300 sec;\n",
            "[2024-01-30 15:56:52,551 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=551)\n",
            "\n",
            "[2024-01-30 15:56:52,552 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 6\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 15:58:44,188 INFO] Step 2800/ 6000; acc: 79.7; ppl:   9.0; xent: 2.2; lr: 0.00167; sents:  110586; bsz: 3105/3704/138; 12600/15031 tok/s;   2497 sec;\n",
            "[2024-01-30 16:01:22,562 INFO] Step 3000/ 6000; acc: 79.9; ppl:   8.9; xent: 2.2; lr: 0.00161; sents:  106559; bsz: 3055/3707/133; 15430/18724 tok/s;   2655 sec;\n",
            "[2024-01-30 16:01:22,581 INFO] Saving checkpoint models/model-med.enfr_step_3000.pt\n",
            "[2024-01-30 16:03:43,874 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=588)\n",
            "\n",
            "[2024-01-30 16:03:43,874 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 7\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 16:04:47,838 INFO] Step 3200/ 6000; acc: 80.3; ppl:   8.8; xent: 2.2; lr: 0.00156; sents:  112718; bsz: 3043/3663/141; 11861/14275 tok/s;   2860 sec;\n",
            "[2024-01-30 16:07:26,326 INFO] Step 3400/ 6000; acc: 81.1; ppl:   8.5; xent: 2.1; lr: 0.00152; sents:  110577; bsz: 3074/3700/138; 15517/18676 tok/s;   3019 sec;\n",
            "[2024-01-30 16:10:04,815 INFO] Step 3600/ 6000; acc: 80.8; ppl:   8.6; xent: 2.1; lr: 0.00147; sents:  106519; bsz: 3064/3713/133; 15466/18744 tok/s;   3177 sec;\n",
            "[2024-01-30 16:13:21,662 INFO] Step 3800/ 6000; acc: 82.0; ppl:   8.1; xent: 2.1; lr: 0.00143; sents:  103711; bsz: 3086/3713/130; 12540/15089 tok/s;   3374 sec;\n",
            "[2024-01-30 16:15:59,953 INFO] Step 4000/ 6000; acc: 81.6; ppl:   8.3; xent: 2.1; lr: 0.00140; sents:  109659; bsz: 3036/3670/137; 15343/18548 tok/s;   3532 sec;\n",
            "[2024-01-30 16:16:00,069 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=6)\n",
            "\n",
            "[2024-01-30 16:16:02,531 INFO] valid stats calculation\n",
            "                           took: 2.5755443572998047 s.\n",
            "[2024-01-30 16:16:02,535 INFO] Train perplexity: 11.7011\n",
            "[2024-01-30 16:16:02,535 INFO] Train accuracy: 75.7696\n",
            "[2024-01-30 16:16:02,535 INFO] Sentences processed: 2.18023e+06\n",
            "[2024-01-30 16:16:02,535 INFO] Average bsz: 3066/3701/136\n",
            "[2024-01-30 16:16:02,535 INFO] Validation perplexity: 8.11076\n",
            "[2024-01-30 16:16:02,535 INFO] Validation accuracy: 82.8859\n",
            "[2024-01-30 16:16:02,535 INFO] Model is improving ppl: 9.01015 --> 8.11076.\n",
            "[2024-01-30 16:16:02,535 INFO] Model is improving acc: 80.6287 --> 82.8859.\n",
            "[2024-01-30 16:16:02,568 INFO] Saving checkpoint models/model-med.enfr_step_4000.pt\n",
            "[2024-01-30 16:17:09,134 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=1133)\n",
            "\n",
            "[2024-01-30 16:17:09,134 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 8\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 16:19:41,236 INFO] Step 4200/ 6000; acc: 82.5; ppl:   8.0; xent: 2.1; lr: 0.00136; sents:  113726; bsz: 3063/3708/142; 11074/13404 tok/s;   3754 sec;\n",
            "[2024-01-30 16:22:20,253 INFO] Step 4400/ 6000; acc: 82.2; ppl:   8.1; xent: 2.1; lr: 0.00133; sents:  110256; bsz: 3045/3695/138; 15321/18587 tok/s;   3913 sec;\n",
            "[2024-01-30 16:23:45,876 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=544)\n",
            "\n",
            "[2024-01-30 16:23:45,876 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 9\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 16:25:38,275 INFO] Step 4600/ 6000; acc: 83.2; ppl:   7.8; xent: 2.0; lr: 0.00130; sents:  105238; bsz: 3117/3717/132; 12591/15016 tok/s;   4111 sec;\n",
            "[2024-01-30 16:28:17,135 INFO] Step 4800/ 6000; acc: 83.1; ppl:   7.7; xent: 2.0; lr: 0.00128; sents:  111713; bsz: 3041/3688/140; 15316/18572 tok/s;   4270 sec;\n",
            "[2024-01-30 16:30:29,493 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=567)\n",
            "\n",
            "[2024-01-30 16:30:29,494 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 10\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 16:31:33,763 INFO] Step 5000/ 6000; acc: 83.2; ppl:   7.7; xent: 2.0; lr: 0.00125; sents:  109111; bsz: 3068/3728/136; 12484/15167 tok/s;   4466 sec;\n",
            "[2024-01-30 16:31:33,785 INFO] Saving checkpoint models/model-med.enfr_step_5000.pt\n",
            "[2024-01-30 16:34:26,103 INFO] Step 5200/ 6000; acc: 83.7; ppl:   7.6; xent: 2.0; lr: 0.00123; sents:  109086; bsz: 3036/3690/136; 14092/17127 tok/s;   4639 sec;\n",
            "[2024-01-30 16:37:05,764 INFO] Step 5400/ 6000; acc: 83.7; ppl:   7.6; xent: 2.0; lr: 0.00120; sents:  110916; bsz: 3100/3706/139; 15534/18569 tok/s;   4798 sec;\n",
            "[2024-01-30 16:37:24,877 INFO] * Transform statistics for med_corpus(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=595)\n",
            "\n",
            "[2024-01-30 16:37:24,877 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* med_corpus: 11\n",
            "\t\t\t* gen_corpus: 1\n",
            "[2024-01-30 16:40:26,647 INFO] Step 5600/ 6000; acc: 84.7; ppl:   7.3; xent: 2.0; lr: 0.00118; sents:  110144; bsz: 3098/3707/138; 12337/14763 tok/s;   4999 sec;\n",
            "[2024-01-30 16:43:06,296 INFO] Step 5800/ 6000; acc: 83.9; ppl:   7.5; xent: 2.0; lr: 0.00116; sents:  107360; bsz: 3027/3680/134; 15166/18439 tok/s;   5159 sec;\n",
            "[2024-01-30 16:46:26,865 INFO] Step 6000/ 6000; acc: 84.9; ppl:   7.3; xent: 2.0; lr: 0.00114; sents:  106375; bsz: 3080/3708/133; 12286/14791 tok/s;   5359 sec;\n",
            "[2024-01-30 16:46:26,984 INFO] * Transform statistics for valid(100.00%):\n",
            "\t\t\t* FilterTooLongStats(filtered=6)\n",
            "\n",
            "[2024-01-30 16:46:29,807 INFO] valid stats calculation\n",
            "                           took: 2.939662456512451 s.\n",
            "[2024-01-30 16:46:29,811 INFO] Train perplexity: 10.155\n",
            "[2024-01-30 16:46:29,811 INFO] Train accuracy: 78.3531\n",
            "[2024-01-30 16:46:29,811 INFO] Sentences processed: 3.27416e+06\n",
            "[2024-01-30 16:46:29,811 INFO] Average bsz: 3067/3702/136\n",
            "[2024-01-30 16:46:29,811 INFO] Validation perplexity: 7.84966\n",
            "[2024-01-30 16:46:29,812 INFO] Validation accuracy: 84.1622\n",
            "[2024-01-30 16:46:29,812 INFO] Model is improving ppl: 8.11076 --> 7.84966.\n",
            "[2024-01-30 16:46:29,812 INFO] Model is improving acc: 82.8859 --> 84.1622.\n",
            "[2024-01-30 16:46:29,842 INFO] Saving checkpoint models/model-med.enfr_step_6000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-med.enfr_step_6000.pt -src corpora/enfr/en-fr-medicine.en-filtered.en.subword.test -output medicine-fr.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "miZt6d4TekUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a348e337-f963-41f9-b4ac-7436201b990a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-30 16:46:57.800454: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 16:46:57.800515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 16:46:57.802323: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 16:46:57.812804: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 16:47:00.165458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-30 16:47:02.366393: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:47:02.367108: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:47:02.367432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-30 16:47:03,021 INFO] Loading checkpoint from models/model-med.enfr_step_6000.pt\n",
            "[2024-01-30 16:47:07,522 INFO] Loading data into the model\n",
            "[2024-01-30 16:48:06,191 INFO] PRED SCORE: -0.2042, PRED PPL: 1.23 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  63.20896553993225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-med.enfr_step_6000.pt -src corpora/enfr/en-fr-technology.en-filtered.en.subword.test -output medicine-fr-technology.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "QQl-x9Yjep21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adca1fe2-d52b-4e01-f2a1-4b4531fbee58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-30 16:48:11.197567: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 16:48:11.197632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 16:48:11.198969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 16:48:11.206266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 16:48:12.508720: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-30 16:48:13.632687: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:48:13.633159: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:48:13.633338: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-30 16:48:14,080 INFO] Loading checkpoint from models/model-med.enfr_step_6000.pt\n",
            "[2024-01-30 16:48:16,898 INFO] Loading data into the model\n",
            "[2024-01-30 16:49:05,727 INFO] PRED SCORE: -0.5991, PRED PPL: 1.82 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  51.70328211784363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!onmt_translate -model models/model-med.enfr_step_6000.pt -src corpora/enfr/en-fr-general.en-filtered.en.subword.test -output medicine-fr-general.translated -gpu 0 -min_length 1"
      ],
      "metadata": {
        "id": "d8HZ8gUDetH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fb1266-3e4d-41e3-b95a-508b6374aa28"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-30 16:49:10.006582: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-30 16:49:10.006647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-30 16:49:10.007948: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-30 16:49:10.015254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-30 16:49:11.294006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-30 16:49:12.834949: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:49:12.835432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-01-30 16:49:12.835601: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "[2024-01-30 16:49:13,522 INFO] Loading checkpoint from models/model-med.enfr_step_6000.pt\n",
            "[2024-01-30 16:49:16,794 INFO] Loading data into the model\n",
            "[2024-01-30 16:50:45,897 INFO] PRED SCORE: -0.3366, PRED PPL: 1.40 NB SENTENCES: 3000\n",
            "Time w/o python interpreter load/terminate:  92.41120862960815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desubword translated files\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model medicine-fr.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model medicine-fr-technology.translated\n",
        "!python3 MT-Preparation/subwording/3-desubword.py target-general.model medicine-fr-general.translated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsrcsNJW5k_q",
        "outputId": "770b8872-5972-4456-f088-d143c52d02dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done desubwording! Output: medicine-fr.translated.desubword\n",
            "Done desubwording! Output: medicine-fr-technology.translated.desubword\n",
            "Done desubwording! Output: medicine-fr-general.translated.desubword\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU scores for medicine-specific NMT model\n",
        "# the filtered test datasets were already desubworded and saved previously when evaluating the baseline model, so we can just reuse them\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-medicine.fr-filtered.fr.subword.test.desubword medicine-fr.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-technology.fr-filtered.fr.subword.test.desubword medicine-fr-technology.translated.desubword\n",
        "!python3 compute-bleu.py corpora/enfr/en-fr-general.fr-filtered.fr.subword.test.desubword medicine-fr-general.translated.desubword"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YLNrjG5pH0",
        "outputId": "c63081a9-46a9-43fd-aab5-c357bdd50d53"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 1st sentence: Cependant, le temps nécessaire à l’ aggravation de la maladie après le traitement était le même dans les deux groupes (environ 10 mois).\n",
            "MTed 1st sentence: Cependant, le délai avant aggravation de la maladie était identique dans les deux groupes (environ 10 mois).\n",
            "BLEU:  54.31552770767254\n",
            "Reference 1st sentence: Crée une nouvelle demande de réunion\n",
            "MTed 1st sentence: ⁇  une nouvelle demande de réunion\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "BLEU:  12.834869950762396\n",
            "Reference 1st sentence: Il ne faut pas oublier que 1 260 détenus marocains sont toujours en captivité dans les geôles du Polisario, et ce depuis plus de 25 ans, en violation flagrante du droit international humanitaire.\n",
            "MTed 1st sentence: Il ne faut pas oublier que 1 260 prisonniers marocaines sont toujours détenus ⁇  dans les prisons du Front POLISARIO, où ils ont été maintenus pendant plus de 25 ans en violation flagrante du droit international humanitaire.\n",
            "BLEU:  41.03778982538722\n"
          ]
        }
      ]
    }
  ]
}